{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_문서요약_Text Summarization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fininsight/text-mining-tutorial/blob/master/4_%EB%AC%B8%EC%84%9C%EC%9A%94%EC%95%BD_Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1Q9mWDrgMUn",
        "colab_type": "text"
      },
      "source": [
        "# 문서 요약 (Text Summarization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeYyuqvEi58W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://www.businessinsider.sg/faang-stocks-are-a-dead-trade-for-the-next-several-months-2018-3/amp/\n",
        "\n",
        "Text = \"The FAANG stocks won’t see much more growth in the near future, according to Bill Studebaker, founder and Chief Investment Officer of Robo Global. \\\n",
        "Studebaker argues we are seeing a 'reallocation' that will continue from large-cap tech stocks into market-weight stocks. \\\n",
        "The FAANG stocks have had a rough few weeks, and have been hit hard since March 12. \\\n",
        "One FAANG to look out for, in the midst of all this, is Amazon, according to Studebaker. \\\n",
        "The stock market is seeing a 'reallocation' out of FAANG stocks, which are not where the smart money is, founder and Chief Investment Officer of Robo Global Bill Studebaker told Business Insider. \\\n",
        "The FAANG stocks (Facebook, Apple, Amazon, Netflix, Google) are all down considerably since March 12, a trend that accelerated when news of a massive Facebook data scandal broke, sending the tech-heavy Nasdaq into a downward frenzy. \\\n",
        "Investors are wondering what’s next. \\\n",
        "And what’s next isn’t good news for FAANG stock optimists, Studebaker thinks. 'This is a dead trade' for the next several months, he said. 'I wouldn’t expect there to be a lot of performance attribution coming from the FAANG stocks,' he added. That is, if the stock market is to see gains in the next several months, they will largely not come from the big tech companies. \\\n",
        "The market is seeing a 'reallocation out of large-cap technology, into other parts of the market,' he said. And this trend could continue for the foreseeable future. 'When you get these reallocation trades, a de-risking, this can go on for months and months.' The FAANG’s are pricey stocks, he said, pointing out that investors will 'factor in the law of big numbers,' he said. 'Just because they’re big cap doesn’t mean they’re safe,' he added. \\\n",
        "Still, he doesn’t necessarily think that investors are going to shift drastically into value stocks. 'With an increasingly favorable macro backdrop, you have strong growth demand.' \\\n",
        "Studebaker, who runs an artificial intelligence and robotics exchange-traded fund with $4 billion in assets under management, thinks that AI and robotics are better areas of growth. His ETF is up 27% in the past year, while the FAANG stocks are also largely up over that same span, even if they are down since March 12. \\\n",
        "While many point to artificial intelligence as an area that will be a boost to Google and Amazon, Studebaker doesn’t see that as a sign of significant growth for the FAANGs. He pointed out that 'eighty to ninety percent of their businesses are still search,' and that 'AI doesn’t really move the needle on the business.' He also said 'the revenue mix [attributable to AI] in those businesses are insignificant.' \\\n",
        "And while he’s not bullish on FAANG’s, he does say that the one FAANG to still watch out for is Amazon, simply because ecommerce still represents a small portion of the global retail market, giving the company room to grow.\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piKvCjRvgHFE",
        "colab_type": "text"
      },
      "source": [
        "#  1 Luhn Summarizer\n",
        "\n",
        "http://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFwBVB3ogZeF",
        "colab_type": "text"
      },
      "source": [
        "### 1) Hans Peter Luhn\n",
        "\n",
        "https://en.wikipedia.org/wiki/Hans_Peter_Luhn\n",
        "\n",
        " Hans Peter Luhn은 공학과 정보과학에서의 개척 작업으로 \"정보 검색의 아버지\"로 알려져 있다. 그는 표제어가 문맥에 포함된 채 배열된 색인(KWIC : keyword-in-context) 개발, 정보 선택 제공(SDI), 완전 텍스트 프로세싱, 자동 발췌(요약), 단어 시소로스의 최초 현대식 사용으로 신뢰를 얻었다. 오늘날 파생된 지식 대부분에는 KWIC 색인이 있으며 과학의 모든 분야에 SDI시스템이 있다. \n",
        "\n",
        "\n",
        " Luhn은 1896년 7월 1일 독일 바르멘에서 태어났다. 아버지가 그 당시 유명한 인쇄업자였으므로, 스위스에서 인쇄업을 배웠다. 어렸을 때무터 그는 창조성이 뛰어난 재능을 보였으며, 기술적 문제, 물리학, 통계학에 관심을 보였다. 그러나 1차 세계대전으로 독일군 통신장교로 복역(1915년1917년)하면서 프랑스 터키 루마니아 불가리아 등지로 옮겨 다녀야만 했다. 1차 대전 이후 그는 스위스의 Gallen 교회 Schweizwrische handels Hochschule로 돌아와 기술, 물리학, 회계분야의 수업을 들었다. 그 후, Luhn은 그리스에서 못다한 공부에 전념했으며, 더블부기기계(Duble-Entry Bookeeping Machine : 카드 대장에 대변기입과 차변 기입을 기록할 수 있는 것)를 발명하였다. 그는 또 Hollerith tabulating/recording machine에 정통했고, 천공카드에서 문자 숫자를 나타내는 장치의 사용을 증가시키게 했다. 1920년부터 1930년까지는 10개의 특허권을 획득하여 그의 탁월한 능력을 보여주었다. 그것들중에 루노메터(Lunometer : 직물의 실길이를 계산하는데 쓰이는 장치)는 지금도 사용되고 있다.\n",
        "\n",
        "\n",
        " 1920년까지 그는 직물 공장에서 일하기 시작했다. 그는 직물 공장의 사업확장을 위해 뉴잉글랜드에서 1924년 미국으로 가게 되었다. 그러나 회사가 곧 파산하였고 Luhn은 직장없이 뉴욕에 남게 되었다. 그는 은행에서 일을 하였고 곧 뉴욕 월스트리트에 소재한 국제어음은행(International Acceptance Bank)에서 재정담당관으로 승진하였다. \n",
        "\n",
        "\n",
        " 1933년 Luhn은 자사인 공학회사 H.P. Luhn & Association을 설립하였고 8년간 자문기술자로 일했다. 1941년 Luhn은 IBM에서 수석 연구기술자로 참여하였고 이후에 정보검색연구 관리자로 일했다. Luhn이 IBM에서 새로운 아이디어를 지속적으로 내놓고 문제를 다르게 접근하여 주목을 받는 동안, 다른 기술자들에게 고차원적인 창조를 하도록 자극하면서 그들의 촉매제 역할을 하여 신뢰를 얻었다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRDgb1Ksd3VH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "\n",
        "# 마침표(.)를 기준으로 문장 분리\n",
        "def get_sentences(txt):\n",
        "    return txt.split('.')\n",
        "\n",
        "# Treebank tokenizer로 토큰으로 분리 \n",
        "def get_words(txt):\n",
        "    return TreebankWordTokenizer().tokenize(txt) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZntB_Sx0d832",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 단어(토큰)의 가중치 계산 및 범위에 포함되는 토큰 식별 \n",
        "def get_keywords(word_list , min_ratio=0.001, max_ratio=0.5) :\n",
        "    assert (min_ratio < 1 and max_ratio < 1)\n",
        "    \n",
        "    # 토큰별로 빈도수 카운팅\n",
        "    count_dict = {}    \n",
        "    for word in word_list:\n",
        "        count_dict.setdefault(word , 0)\n",
        "        count_dict[word] +=1\n",
        "    #print(count_dict)\n",
        "    #print(len(word_list))\n",
        "    \n",
        "    # 분석 문서의 총 토큰수 대비 해당 토큰의 빈도 비율\n",
        "    keywords = set()\n",
        "    for word , cnt in count_dict.items():\n",
        "        word_percentage = count_dict[word]* 1.0 / len (word_list)\n",
        "        #print(word_percentage)\n",
        "        \n",
        "        # 사전 정의한 비율내에 포함 된 경우 키워드에 추가\n",
        "        if word_percentage <= max_ratio and word_percentage >=min_ratio:\n",
        "            keywords.add(word)\n",
        "    return keywords\n",
        "        \n",
        "\n",
        "# 문장의 가중치 계산\n",
        "def get_sentence_weight (sentence , keywords):\n",
        "    sen_list = sentence.split(' ')\n",
        "    window_start = 0; window_end = -1;\n",
        "    \n",
        "    # 문장내에서 윈도 시작 위치 탐색\n",
        "    # 범위내 속한 키워드가 등장하는 첫번째 위치 계산\n",
        "    for i in range(len(sen_list)):\n",
        "        if sen_list[i] in keywords:\n",
        "            window_start = i\n",
        "            break\n",
        "    \n",
        "    # 문장내에서 윈도 종료 위치 탐색\n",
        "    # 범위내 속한 키워드가 등장하는 마지막 위치 계산\n",
        "    for i in range(len(sen_list) - 1 , 0 , -1) :\n",
        "        if sen_list[i] in keywords:\n",
        "            window_end = i\n",
        "            break\n",
        "    \n",
        "    # 윈도의 시작위치가 종료위치보다 큰경우 => 분석할 단어(토큰)가 없는 경우 종료\n",
        "    if window_start > window_end :\n",
        "        return 0\n",
        "    \n",
        "    # 윈도 크기 계산\n",
        "    window_size = window_end - window_start + 1\n",
        "    \n",
        "    # 분석 대상 문장 중 범위(0.001 ~ 0.5)에 포함된 토큰 개수 카운팅\n",
        "    keywords_cnt =0\n",
        "    for w in sen_list :\n",
        "        if w in keywords:\n",
        "            keywords_cnt +=1\n",
        "    \n",
        "    # (분석 대상 문장 중 범위(0.001 ~ 0.5)에 포함된 토큰 개수) / 윈도사이즈\n",
        "    return keywords_cnt*keywords_cnt *1.0 / window_size\n",
        "\n",
        "  \n",
        "# 문서 요약\n",
        "def summarize(content ,max_no_of_sentences = 10):\n",
        "    \n",
        "    txt = content\n",
        "    # 단어(토큰) 분리\n",
        "    word_list = get_words(txt)\n",
        "    \n",
        "    # 단어(토큰) 가중치 계산 및 범위 내 포함 단어(토큰) 추출\n",
        "    keywords = get_keywords(word_list , 0.01 , 0.5)\n",
        "    print(keywords)\n",
        "    \n",
        "    sentence_list = get_sentences(txt)\n",
        "    sentence_weight = []\n",
        "\n",
        "    # 문장별 가중치 계산\n",
        "    for sen in sentence_list :\n",
        "        sentence_weight.append ((get_sentence_weight(sen , keywords) ,sen))\n",
        "           \n",
        "    # 문장별 가중치 역순 계산\n",
        "    sentence_weight.sort(reverse = True)\n",
        "    #print(sentence_weight)\n",
        "    ret_list = []\n",
        "    ret_cnt = min(max_no_of_sentences  , len(sentence_list))\n",
        "    \n",
        "    for i in range (ret_cnt) :\n",
        "        ret_list.append (str(sentence_weight[i][0])  + ' : ' + sentence_weight[i][1] +'.')\n",
        "    \n",
        "    return ret_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmZxMoce6S3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "0375f3d1-ed16-4fbc-d0c0-f23724ccc4cf"
      },
      "source": [
        "slist = summarize (Text ,  3)\n",
        "for s in slist :\n",
        "    print(s)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'that', 'The', 'the', 'are', 'stocks', 'for', 'to', 't', 'of', 'a', 'he', '’', 'in', 'and', 'is', 'FAANG', \"'\", ',', 'out', 'Studebaker'}\n",
            "5.0625 :  One FAANG to look out for, in the midst of all this, is Amazon, according to Studebaker.\n",
            "4.9 : ' And while he’s not bullish on FAANG’s, he does say that the one FAANG to still watch out for is Amazon, simply because ecommerce still represents a small portion of the global retail market, giving the company room to grow.\n",
            "4.761904761904762 : ' The FAANG’s are pricey stocks, he said, pointing out that investors will 'factor in the law of big numbers,' he said.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctQ8vU85d2Uy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUOOmfhngBnb",
        "colab_type": "text"
      },
      "source": [
        "# 2 Textrank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2cG-zDxW0aW",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://www.researchgate.net/profile/Khushboo_Thakkar3/publication/232645575/figure/fig1/AS:575720050573312@1514273764062/Sample-graph-build-for-sentence-extraction.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-wrC-1D_a0V",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 TextRank 직접 구현하기 (Matrix 활용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlOhA83gBcFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "import re\n",
        "from operator import itemgetter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5uS2EJPBvn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 코사인 유사도 (1. 단어의 표현 예제 참고)\n",
        "def cosine_similarity(x, y):\n",
        "    # x와 y, 두 벡터의 코사인 유사도를 계산하는 함수\n",
        "    nominator = np.dot(x, y)    # 분자\n",
        "    denominator = np.linalg.norm(x)*np.linalg.norm(y)  # 분모\n",
        "    return nominator/denominator\n",
        "  \n",
        "cosine_similarity([0,0,1,0],[0,0,1,1])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev4s4s0YBxa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 문장간 유사도 측정 (BoW를 활용 코사인 유사도 측정)\n",
        "def sentence_similarity(sentence1, sentence2):\n",
        "    # 각 문장을 소문자로 변환\n",
        "    sentence1 = [word.lower() for word in sentence1.split()]\n",
        "    sentence2 = [word.lower() for word in sentence2.split()]\n",
        "    \n",
        "    # BoW 생성을 위한 unique한 단어로 배열 생성\n",
        "    words_ls = list(set(sentence1 + sentence2))\n",
        " \n",
        "    bow1 = [0] * len(words_ls)\n",
        "    bow2 = [0] * len(words_ls)\n",
        " \n",
        "    # 첫번째 문장 BoW 생성\n",
        "    for word in sentence1:\n",
        "        bow1[words_ls.index(word)] += 1\n",
        "\n",
        "    # 두번째 문장 BoW 생성\n",
        "    for word in sentence2:\n",
        "        bow2[words_ls.index(word)] += 1\n",
        " \n",
        "    return cosine_similarity(bow1, bow2)\n",
        "\n",
        "sentence_similarity('나는 치킨을 좋아해','나는 치킨을 싫어해')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38ThXVuHB2bA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def buildMatrix(sentences):\n",
        "    # 문장별로 그래프 edge를 Matrix 형태로 생성\n",
        "    weighted_edge = np.zeros((len(sentences), len(sentences)),dtype=np.float32)\n",
        "    \n",
        "    #print(len(weighted_edge))\n",
        "    #print(len(sentences))\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        for j in range(len(sentences)):\n",
        "            if i == j:\n",
        "                continue\n",
        "            weighted_edge[i][j] = sentence_similarity(sentences[i], sentences[j])\n",
        "            #print('[' + str(i) + ',' + str(j)+'] = ' + str(weighted_edge[i][j]))\n",
        " \n",
        "    # normalize \n",
        "    for i in range(len(weighted_edge)):\n",
        "        weighted_edge[i] /= weighted_edge[i].sum()\n",
        " \n",
        "    return weighted_edge\n",
        " \n",
        "S = buildMatrix(sent_tokenize(Text))    \n",
        "print(S[:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlAJ0nVaB36Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scoring(A, eps=0.0001, d=0.85, max_iter = 50):\n",
        "    P = np.ones(len(A)) / len(A)\n",
        "    for iter in range(0,max_iter):\n",
        "        #print(iter)\n",
        "        newP = np.ones(len(A)) * (1 - d) / len(A) + d * A.T.dot(P)\n",
        "        \n",
        "        if abs((newP - P).sum()) <= eps:\n",
        "            return newP\n",
        "        P = newP"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydkPRpImN-Zl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = scoring(S)\n",
        "a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DVvQdjSB5PE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summarize(text, linesinSummary=10):\n",
        "  text = sent_tokenize(text)\n",
        "  weighted_edge = buildMatrix(text) \n",
        "  score = scoring(weighted_edge)\n",
        "  #print(score) \n",
        "\n",
        "  rankedSentenceIndexes = [item[0] for item in sorted(enumerate(score), key=lambda item: -item[1])]\n",
        "  selectedSentences = sorted(rankedSentenceIndexes[:linesinSummary])\n",
        "  summary = itemgetter(*selectedSentences)(text)\n",
        "  return summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqjLqMOyR18k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summarize(Text,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J16o9Y89WgNT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8Yj0ioJimhl",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 TextRank 직접 구현하기 (Graph 활용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBBP3udO-rWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import networkx as nx\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "def sentences(text):\n",
        "    return sent_tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuOJL2EW-uII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def connect(nodes):\n",
        "    return [(start,end ,sentence_similarity(start, end)) for start in nodes for end in nodes if start is not end]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jFFd2ka-t9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def similarity(c1,c2):\n",
        "#    return len(common_words(c1, c2))/(math.log(len(c1))+math.log(len(c2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG9S-Fxw-tzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def common_words(c1,c2):\n",
        "#    elem1 = [x for x in c1.split()]\n",
        "#    elem2 = [x for x in c2.split()]\n",
        "#    c3=list()\n",
        "#    for item in elem1:\n",
        "#        if item in elem2:\n",
        "#            c3.append(item)\n",
        "#    return c3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKEErtAU-tmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rank(nodes,edges):\n",
        "    graph=nx.diamond_graph()\n",
        "    graph.add_nodes_from(nodes)\n",
        "    graph.add_weighted_edges_from(edges)\n",
        "    return nx.pagerank(graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XQkxYkv-zxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summarize(text,num_summaries=6):\n",
        "    nodes=sentences(text)\n",
        "    edges=connect(nodes)\n",
        "    scores=rank(nodes,edges)\n",
        "    #print(nodes)\n",
        "    return sorted(scores,key=scores.get)[:num_summaries]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwlnlxyP8VM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary=summarize(Text, 3)\n",
        "\n",
        "for sent in summary :\n",
        "  print(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPayQbC4iqmB",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 gensim을 활용한 요약"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-GdO7EigEg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.summarization.summarizer import summarize\n",
        "print(summarize(Text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_1gyP1uX2z1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}