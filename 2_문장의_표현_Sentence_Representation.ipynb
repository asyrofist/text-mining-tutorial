{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_문장의 표현_Sentence Representation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fininsight/text-mining-tutorial/blob/master/2_%EB%AC%B8%EC%9E%A5%EC%9D%98_%ED%91%9C%ED%98%84_Sentence_Representation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zEFesPBvXe2C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 문장의 표현 (Sentence Representation)"
      ]
    },
    {
      "metadata": {
        "id": "52uiZhBWaR4M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 BoW (Bag of Words)"
      ]
    },
    {
      "metadata": {
        "id": "xuz1lvCi_e-y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://image.slidesharecdn.com/vector-space-models-170118145044/95/cs571-vector-space-models-3-638.jpg?cb=1485433004\" />\n",
        "\n",
        "https://en.wikipedia.org/wiki/Bag-of-words_model\n",
        "https://www.slideshare.net/jchoi7s/cs571-vector-space-models"
      ]
    },
    {
      "metadata": {
        "id": "jUABPDuYAO7Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 동물원 예제"
      ]
    },
    {
      "metadata": {
        "id": "dPZCmyM7aR4O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence_ls = [\n",
        " '오늘 동물원에서 코끼리를 봤어',\n",
        " '오늘 동물원에서 원숭이에게 사과를 줬어'   \n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zuMcIp6_aR4R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1) 띄어쓰기 단위로 토큰화"
      ]
    },
    {
      "metadata": {
        "id": "HK8UIQfKaR4S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence_ls = [sentence.split() for sentence in sentence_ls]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-E2fakIGaR4U",
        "colab_type": "code",
        "outputId": "0fc42440-ed4a-4c19-a5e2-124c40668048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "sentence_ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['오늘', '동물원에서', '코끼리를', '봤어'], ['오늘', '동물원에서', '원숭이에게', '사과를', '줬어']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "vxOK8R52aR4X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2) 각 고유 토큰에 인덱스(Index)를 지정"
      ]
    },
    {
      "metadata": {
        "id": "HjQvx_d1aR4Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "token_dict = defaultdict(lambda : len(token_dict))\n",
        "\n",
        "for sentence in sentence_ls:\n",
        "    for token in sentence:\n",
        "        token_dict[token]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qHzASAoCaR4a",
        "colab_type": "code",
        "outputId": "e5215068-81b3-41a7-a6c6-9459064c6da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "cell_type": "code",
      "source": [
        "token_dict"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.<lambda>>,\n",
              "            {'동물원에서': 1,\n",
              "             '봤어': 3,\n",
              "             '사과를': 5,\n",
              "             '오늘': 0,\n",
              "             '원숭이에게': 4,\n",
              "             '줬어': 6,\n",
              "             '코끼리를': 2})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "BBi-jFhVaR4c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3) 토큰 인덱스 정렬"
      ]
    },
    {
      "metadata": {
        "id": "Qn3hkjJuaR4d",
        "colab_type": "code",
        "outputId": "8b622e68-6add-46d4-a262-e25e2e156bb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "cell_type": "code",
      "source": [
        "index_token_ls = sorted((value, key) for key, value in token_dict.items())\n",
        "index_token_ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, '오늘'),\n",
              " (1, '동물원에서'),\n",
              " (2, '코끼리를'),\n",
              " (3, '봤어'),\n",
              " (4, '원숭이에게'),\n",
              " (5, '사과를'),\n",
              " (6, '줬어')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "qAg_or1naR4f",
        "colab_type": "code",
        "outputId": "d3938aae-4719-494f-fd2c-ea33bc6559d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "token_in_order = [tup[1] for tup in index_token_ls]\n",
        "token_in_order"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['오늘', '동물원에서', '코끼리를', '봤어', '원숭이에게', '사과를', '줬어']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "3ELkZo_BaR4h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4) 빈(empty) BOW 생성"
      ]
    },
    {
      "metadata": {
        "id": "ct2qziLJaR4i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "n_words = len(token_dict) # 전체 고유 토큰의 수\n",
        "n_sentence = len(sentence_ls) # 전체 문장의 수\n",
        "\n",
        "BOW = pd.DataFrame(\n",
        "    np.zeros((n_sentence, n_words)),\n",
        "    columns = token_in_order,\n",
        "    index = ['문장_1', '문장_2'],\n",
        "    dtype = int,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q8-PznHAaR4k",
        "colab_type": "code",
        "outputId": "0200aaee-ba6b-4e4e-84e9-b77b30c9565c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "BOW"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>오늘</th>\n",
              "      <th>동물원에서</th>\n",
              "      <th>코끼리를</th>\n",
              "      <th>봤어</th>\n",
              "      <th>원숭이에게</th>\n",
              "      <th>사과를</th>\n",
              "      <th>줬어</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>문장_1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>문장_2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      오늘  동물원에서  코끼리를  봤어  원숭이에게  사과를  줬어\n",
              "문장_1   0      0     0   0      0    0   0\n",
              "문장_2   0      0     0   0      0    0   0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "G7cZKHjeaR4n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5) 각 토큰을 BOW에 하나씩 담는다."
      ]
    },
    {
      "metadata": {
        "id": "Jd4Gyeh4aR4n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(sentence_ls):\n",
        "    for token in sentence:\n",
        "        \n",
        "        token_location = token_dict[token] # 해당 토큰의 위치(column)\n",
        "        BOW.iloc[i, token_location] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CSJ2jydTaR4p",
        "colab_type": "code",
        "outputId": "7227ed3e-4c1b-4884-9619-a759bf701069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "BOW"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>오늘</th>\n",
              "      <th>동물원에서</th>\n",
              "      <th>코끼리를</th>\n",
              "      <th>봤어</th>\n",
              "      <th>원숭이에게</th>\n",
              "      <th>사과를</th>\n",
              "      <th>줬어</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>문장_1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>문장_2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      오늘  동물원에서  코끼리를  봤어  원숭이에게  사과를  줬어\n",
              "문장_1   1      1     1   1      0    0   0\n",
              "문장_2   1      1     0   0      1    1   1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "hx3EGCegbh8f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CHnBJ8wuaR4t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 양념치킨과 후라이드치킨 예제"
      ]
    },
    {
      "metadata": {
        "id": "4UZDtYS7aR4u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence_ls = [\n",
        "'나는 양념 치킨을 좋아해 하지만 후라이드 치킨을 싫어해',\n",
        "'나는 후라이드 치킨을 좋아해 하지만 양념 치킨을 싫어해'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CmXVteHEaR4x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1) 띄어쓰기 단위로 토큰화"
      ]
    },
    {
      "metadata": {
        "id": "TO8Vnn2SaR4y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence_ls = [sentence.split() for sentence in sentence_ls]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LTMWP5GjaR40",
        "colab_type": "code",
        "outputId": "d848d480-ed5b-4903-e628-8dc7a482a85a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "sentence_ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['나는', '양념', '치킨을', '좋아해', '하지만', '후라이드', '치킨을', '싫어해'],\n",
              " ['나는', '후라이드', '치킨을', '좋아해', '하지만', '양념', '치킨을', '싫어해']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "edQ76L6EaR44",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2) 각 고유 토큰에 인덱스(Index)를 지정"
      ]
    },
    {
      "metadata": {
        "id": "6GZSTsvSaR44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "token_dict = defaultdict(lambda : len(token_dict))\n",
        "\n",
        "for sentence in sentence_ls:\n",
        "    for token in sentence:\n",
        "        token_dict[token]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fLXtAuLEaR46",
        "colab_type": "code",
        "outputId": "2bdac26b-1268-4cbc-a26a-48ba64a0886a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "cell_type": "code",
      "source": [
        "token_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.<lambda>>,\n",
              "            {'나는': 0,\n",
              "             '싫어해': 6,\n",
              "             '양념': 1,\n",
              "             '좋아해': 3,\n",
              "             '치킨을': 2,\n",
              "             '하지만': 4,\n",
              "             '후라이드': 5})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "BNKgIOkdaR4-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3) 토큰 인덱스 정렬"
      ]
    },
    {
      "metadata": {
        "id": "H3Sk7-lJaR4-",
        "colab_type": "code",
        "outputId": "28007099-efcc-420a-c733-b4f387d705ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "cell_type": "code",
      "source": [
        "index_token_ls = sorted((value, key) for key, value in token_dict.items())\n",
        "index_token_ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, '나는'),\n",
              " (1, '양념'),\n",
              " (2, '치킨을'),\n",
              " (3, '좋아해'),\n",
              " (4, '하지만'),\n",
              " (5, '후라이드'),\n",
              " (6, '싫어해')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "BEBwPXl7aR5A",
        "colab_type": "code",
        "outputId": "526890fa-be4e-47aa-df65-b17f88922c42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "token_in_order = [tup[1] for tup in index_token_ls]\n",
        "token_in_order"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['나는', '양념', '치킨을', '좋아해', '하지만', '후라이드', '싫어해']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "QmgGxyL7aR5E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 4) 빈(empty) BOW 생성"
      ]
    },
    {
      "metadata": {
        "id": "_pmg9xo2aR5F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "n_words = len(token_dict) # 전체 고유 토큰의 수\n",
        "n_sentence = len(sentence_ls) # 전체 문장의 수\n",
        "\n",
        "BOW = pd.DataFrame(\n",
        "    np.zeros((n_sentence, n_words)),\n",
        "    columns = token_in_order,\n",
        "    index = ['문장_1', '문장_2'],\n",
        "    dtype = int,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hNj8f-lvaR5H",
        "colab_type": "code",
        "outputId": "f46dce9f-30a0-4f3f-ccc5-1f9afe8e5ddc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "BOW"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>나는</th>\n",
              "      <th>양념</th>\n",
              "      <th>치킨을</th>\n",
              "      <th>좋아해</th>\n",
              "      <th>하지만</th>\n",
              "      <th>후라이드</th>\n",
              "      <th>싫어해</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>문장_1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>문장_2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      나는  양념  치킨을  좋아해  하지만  후라이드  싫어해\n",
              "문장_1   0   0    0    0    0     0    0\n",
              "문장_2   0   0    0    0    0     0    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "HiQkXwbkaR5J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 5) 각 토큰을 BOW에 하나씩 담는다."
      ]
    },
    {
      "metadata": {
        "id": "hIICxX7DaR5K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(sentence_ls):\n",
        "    for token in sentence:\n",
        "        \n",
        "        token_location = token_dict[token] # 해당 토큰의 위치(column)\n",
        "        BOW.iloc[i, token_location] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zL3v6nnjaR5M",
        "colab_type": "code",
        "outputId": "c24806e3-24a3-4a33-bfc1-dabdc5b945bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "BOW"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>나는</th>\n",
              "      <th>양념</th>\n",
              "      <th>치킨을</th>\n",
              "      <th>좋아해</th>\n",
              "      <th>하지만</th>\n",
              "      <th>후라이드</th>\n",
              "      <th>싫어해</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>문장_1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>문장_2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      나는  양념  치킨을  좋아해  하지만  후라이드  싫어해\n",
              "문장_1   1   1    2    1    1     1    1\n",
              "문장_2   1   1    2    1    1     1    1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "491wb8Avb4ij",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "C1bFCpsIaSCU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 CBoW (Continious Bag of Words)"
      ]
    },
    {
      "metadata": {
        "id": "SUmInaheaSCV",
        "colab_type": "code",
        "outputId": "bd6bac4f-cc4d-4817-d4bb-dd5ee5208afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9ab32c83b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "vnX1M86_aSCX",
        "colab_type": "code",
        "outputId": "fa5ff4b8-88af-4524-f9e0-33444a279d0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
        "\n",
        "\n",
        "vocab = list(set(sentence))\n",
        "vocab_size = len(vocab)\n",
        "vocab_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "o-Q8PT-zaSCa",
        "colab_type": "code",
        "outputId": "2ee26b71-0ca2-419b-c829-64aab87f33fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "data = \\\n",
        "[([sentence[idx-2],sentence[idx-1],sentence[idx+1],sentence[idx+2]],sentence[idx])\\\n",
        " for idx in range(2,len(sentence)-2)]\n",
        "print(data[:3],end='\\n\\n\\n')\n",
        "\n",
        "word_to_ix = {val : idx for idx,val in enumerate(vocab)}\n",
        "print(word_to_ix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['When', 'forty', 'shall', 'besiege'], 'winters'), (['forty', 'winters', 'besiege', 'thy'], 'shall'), (['winters', 'shall', 'thy', 'brow,'], 'besiege')]\n",
            "\n",
            "\n",
            "{'How': 0, 'lies,': 1, 'livery': 2, 'treasure': 3, 'small': 4, 'gazed': 5, 'warm': 6, 'all': 7, 'shame,': 8, 'use,': 9, 'by': 10, 'fair': 11, 'blood': 12, 'were': 13, 'dig': 14, 'thriftless': 15, 'worth': 16, 'new': 17, \"beauty's\": 18, 'Were': 19, 'being': 20, 'the': 21, 'sunken': 22, 'his': 23, 'thy': 24, \"totter'd\": 25, 'deep': 26, \"feel'st\": 27, 'And': 28, 'when': 29, \"deserv'd\": 30, 'child': 31, 'forty': 32, 'besiege': 33, 'see': 34, 'When': 35, 'beauty': 36, 'To': 37, 'weed': 38, 'eyes,': 39, \"excuse,'\": 40, 'brow,': 41, 'now,': 42, 'shall': 43, \"youth's\": 44, 'on': 45, 'old,': 46, 'lusty': 47, 'praise': 48, 'couldst': 49, 'Thy': 50, 'art': 51, 'This': 52, 'all-eating': 53, 'and': 54, 'made': 55, 'make': 56, 'in': 57, 'Will': 58, 'an': 59, 'where': 60, 'it': 61, 'sum': 62, 'thou': 63, 'proud': 64, 'held:': 65, \"'This\": 66, 'thine!': 67, 'succession': 68, 'If': 69, 'Where': 70, 'field,': 71, 'within': 72, 'of': 73, 'Proving': 74, 'praise.': 75, 'my': 76, 'much': 77, 'trenches': 78, 'mine': 79, 'Then': 80, 'own': 81, 'thine': 82, 'days;': 83, 'so': 84, 'old': 85, 'say,': 86, 'Shall': 87, 'winters': 88, 'answer': 89, 'more': 90, 'count,': 91, 'be': 92, 'asked,': 93, 'to': 94, 'cold.': 95, 'a': 96}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bi79GEB9aSCc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CBOW(nn.Module) : \n",
        "    \n",
        "    def __init__(self,vocab_size, embedding_dim):  \n",
        "        \n",
        "        super(CBOW,self).__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = 10#embedding_size\n",
        "        self.embeddings = nn.Embedding(self.vocab_size,self.embedding_dim)\n",
        "        self.linear1 = nn.Linear(self.embedding_dim,128)\n",
        "        self.linear2 = nn.Linear(128,self.vocab_size)\n",
        "    \n",
        "    def forward(self,inputs) : \n",
        "        self.embeds = self.embeddings(inputs).sum(dim=0).unsqueeze(0)\n",
        "        out = F.relu(self.linear1(self.embeds))\n",
        "        out = self.linear2(out)\n",
        "            \n",
        "        log_probs = F.log_softmax(out,dim=1)\n",
        "        return log_probs\n",
        "    \n",
        "    def get_word_vector(self,target,word_to_ix) : \n",
        "        word2id = torch.LongTensor([word_to_ix[target]])\n",
        "        return self.embeddings(word2id).view(1, -1)\n",
        "        \n",
        "    \n",
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    word2id = torch.tensor(idxs, dtype=torch.long)\n",
        "    return word2id\n",
        "\n",
        "def make_target_vector(target, word_to_ix):\n",
        "    idxs = [word_to_ix[target]]\n",
        "    return torch.LongTensor(idxs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3lfwLI8WaSCf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 10\n",
        "EPOCH = 20\n",
        "VERVOSE = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e6s_6R-kaSCh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_cbow(vocab_size , word_to_ix) :\n",
        "    loss_function = nn.NLLLoss()\n",
        "    model = CBOW(vocab_size, EMBEDDING_DIM)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(EPOCH):\n",
        "        total_loss = 0\n",
        "        for context, target in data:\n",
        "\n",
        "            # 아래의 프로세스는 word : ID 로 converting 시켜주는 process 이다.\n",
        "            context_idxs = make_context_vector(context,word_to_ix)\n",
        "            target_idxs = torch.LongTensor([word_to_ix[target]])\n",
        "\n",
        "            # pytorch는 gradient 를 누진적으로 계산하기 때문에, 0으로 만들어주어야한다.\n",
        "            model.zero_grad()\n",
        "\n",
        "            # input 값을 넣으면 log_probs 라는 output 값이 나온다.\n",
        "            log_probs = model(context_idxs)\n",
        "\n",
        "            # 이 값을 위에서 정의한 손실 함수를 기반으로 loss 를 계산한다.\n",
        "            loss = loss_function(log_probs, target_idxs)\n",
        "\n",
        "            # 위에서 나온 loss를 기반으로 back propagation 을 돌린다.\n",
        "            # 또한, optimizer 를 update 하면서 parameter 또한 update 한다.\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss\n",
        "\n",
        "            if epoch % VERVOSE == 0 : \n",
        "                loss_avg = float(total_loss / len(data))\n",
        "                print(\"{}/{} loss {:.2f}\".format(epoch, EPOCH, loss_avg))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZRUoUQRYaSCj",
        "colab_type": "code",
        "outputId": "f35073aa-928d-4b02-967d-50ce81515c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8099
        }
      },
      "cell_type": "code",
      "source": [
        "run_cbow(vocab_size , word_to_ix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0/20 loss 0.04\n",
            "0/20 loss 0.09\n",
            "0/20 loss 0.13\n",
            "0/20 loss 0.17\n",
            "0/20 loss 0.21\n",
            "0/20 loss 0.25\n",
            "0/20 loss 0.30\n",
            "0/20 loss 0.34\n",
            "0/20 loss 0.38\n",
            "0/20 loss 0.41\n",
            "0/20 loss 0.45\n",
            "0/20 loss 0.50\n",
            "0/20 loss 0.54\n",
            "0/20 loss 0.59\n",
            "0/20 loss 0.64\n",
            "0/20 loss 0.68\n",
            "0/20 loss 0.71\n",
            "0/20 loss 0.76\n",
            "0/20 loss 0.81\n",
            "0/20 loss 0.85\n",
            "0/20 loss 0.89\n",
            "0/20 loss 0.92\n",
            "0/20 loss 0.95\n",
            "0/20 loss 1.00\n",
            "0/20 loss 1.04\n",
            "0/20 loss 1.08\n",
            "0/20 loss 1.12\n",
            "0/20 loss 1.17\n",
            "0/20 loss 1.23\n",
            "0/20 loss 1.27\n",
            "0/20 loss 1.31\n",
            "0/20 loss 1.35\n",
            "0/20 loss 1.39\n",
            "0/20 loss 1.43\n",
            "0/20 loss 1.48\n",
            "0/20 loss 1.52\n",
            "0/20 loss 1.55\n",
            "0/20 loss 1.60\n",
            "0/20 loss 1.65\n",
            "0/20 loss 1.69\n",
            "0/20 loss 1.73\n",
            "0/20 loss 1.77\n",
            "0/20 loss 1.81\n",
            "0/20 loss 1.85\n",
            "0/20 loss 1.90\n",
            "0/20 loss 1.94\n",
            "0/20 loss 1.98\n",
            "0/20 loss 2.03\n",
            "0/20 loss 2.07\n",
            "0/20 loss 2.10\n",
            "0/20 loss 2.15\n",
            "0/20 loss 2.19\n",
            "0/20 loss 2.23\n",
            "0/20 loss 2.27\n",
            "0/20 loss 2.31\n",
            "0/20 loss 2.36\n",
            "0/20 loss 2.40\n",
            "0/20 loss 2.44\n",
            "0/20 loss 2.49\n",
            "0/20 loss 2.54\n",
            "0/20 loss 2.59\n",
            "0/20 loss 2.63\n",
            "0/20 loss 2.68\n",
            "0/20 loss 2.72\n",
            "0/20 loss 2.76\n",
            "0/20 loss 2.81\n",
            "0/20 loss 2.85\n",
            "0/20 loss 2.89\n",
            "0/20 loss 2.93\n",
            "0/20 loss 2.97\n",
            "0/20 loss 3.02\n",
            "0/20 loss 3.06\n",
            "0/20 loss 3.10\n",
            "0/20 loss 3.15\n",
            "0/20 loss 3.20\n",
            "0/20 loss 3.24\n",
            "0/20 loss 3.28\n",
            "0/20 loss 3.31\n",
            "0/20 loss 3.35\n",
            "0/20 loss 3.40\n",
            "0/20 loss 3.44\n",
            "0/20 loss 3.48\n",
            "0/20 loss 3.52\n",
            "0/20 loss 3.56\n",
            "0/20 loss 3.61\n",
            "0/20 loss 3.65\n",
            "0/20 loss 3.69\n",
            "0/20 loss 3.73\n",
            "0/20 loss 3.76\n",
            "0/20 loss 3.79\n",
            "0/20 loss 3.84\n",
            "0/20 loss 3.88\n",
            "0/20 loss 3.93\n",
            "0/20 loss 3.97\n",
            "0/20 loss 4.01\n",
            "0/20 loss 4.06\n",
            "0/20 loss 4.11\n",
            "0/20 loss 4.15\n",
            "0/20 loss 4.19\n",
            "0/20 loss 4.23\n",
            "0/20 loss 4.27\n",
            "0/20 loss 4.31\n",
            "0/20 loss 4.35\n",
            "0/20 loss 4.39\n",
            "0/20 loss 4.44\n",
            "0/20 loss 4.48\n",
            "0/20 loss 4.52\n",
            "0/20 loss 4.56\n",
            "0/20 loss 4.61\n",
            "0/20 loss 4.66\n",
            "0/20 loss 4.70\n",
            "5/20 loss 0.04\n",
            "5/20 loss 0.08\n",
            "5/20 loss 0.12\n",
            "5/20 loss 0.16\n",
            "5/20 loss 0.20\n",
            "5/20 loss 0.23\n",
            "5/20 loss 0.27\n",
            "5/20 loss 0.31\n",
            "5/20 loss 0.34\n",
            "5/20 loss 0.37\n",
            "5/20 loss 0.41\n",
            "5/20 loss 0.45\n",
            "5/20 loss 0.49\n",
            "5/20 loss 0.54\n",
            "5/20 loss 0.58\n",
            "5/20 loss 0.62\n",
            "5/20 loss 0.66\n",
            "5/20 loss 0.70\n",
            "5/20 loss 0.75\n",
            "5/20 loss 0.79\n",
            "5/20 loss 0.83\n",
            "5/20 loss 0.85\n",
            "5/20 loss 0.89\n",
            "5/20 loss 0.93\n",
            "5/20 loss 0.97\n",
            "5/20 loss 1.01\n",
            "5/20 loss 1.05\n",
            "5/20 loss 1.09\n",
            "5/20 loss 1.14\n",
            "5/20 loss 1.19\n",
            "5/20 loss 1.22\n",
            "5/20 loss 1.26\n",
            "5/20 loss 1.30\n",
            "5/20 loss 1.34\n",
            "5/20 loss 1.39\n",
            "5/20 loss 1.42\n",
            "5/20 loss 1.45\n",
            "5/20 loss 1.50\n",
            "5/20 loss 1.54\n",
            "5/20 loss 1.58\n",
            "5/20 loss 1.62\n",
            "5/20 loss 1.66\n",
            "5/20 loss 1.69\n",
            "5/20 loss 1.73\n",
            "5/20 loss 1.78\n",
            "5/20 loss 1.82\n",
            "5/20 loss 1.86\n",
            "5/20 loss 1.90\n",
            "5/20 loss 1.94\n",
            "5/20 loss 1.97\n",
            "5/20 loss 2.01\n",
            "5/20 loss 2.05\n",
            "5/20 loss 2.09\n",
            "5/20 loss 2.12\n",
            "5/20 loss 2.16\n",
            "5/20 loss 2.20\n",
            "5/20 loss 2.25\n",
            "5/20 loss 2.29\n",
            "5/20 loss 2.33\n",
            "5/20 loss 2.38\n",
            "5/20 loss 2.42\n",
            "5/20 loss 2.46\n",
            "5/20 loss 2.50\n",
            "5/20 loss 2.54\n",
            "5/20 loss 2.58\n",
            "5/20 loss 2.62\n",
            "5/20 loss 2.65\n",
            "5/20 loss 2.69\n",
            "5/20 loss 2.73\n",
            "5/20 loss 2.77\n",
            "5/20 loss 2.80\n",
            "5/20 loss 2.84\n",
            "5/20 loss 2.87\n",
            "5/20 loss 2.92\n",
            "5/20 loss 2.96\n",
            "5/20 loss 3.00\n",
            "5/20 loss 3.04\n",
            "5/20 loss 3.07\n",
            "5/20 loss 3.11\n",
            "5/20 loss 3.15\n",
            "5/20 loss 3.19\n",
            "5/20 loss 3.23\n",
            "5/20 loss 3.26\n",
            "5/20 loss 3.30\n",
            "5/20 loss 3.35\n",
            "5/20 loss 3.39\n",
            "5/20 loss 3.43\n",
            "5/20 loss 3.46\n",
            "5/20 loss 3.49\n",
            "5/20 loss 3.52\n",
            "5/20 loss 3.56\n",
            "5/20 loss 3.60\n",
            "5/20 loss 3.65\n",
            "5/20 loss 3.69\n",
            "5/20 loss 3.73\n",
            "5/20 loss 3.77\n",
            "5/20 loss 3.81\n",
            "5/20 loss 3.85\n",
            "5/20 loss 3.89\n",
            "5/20 loss 3.92\n",
            "5/20 loss 3.96\n",
            "5/20 loss 3.99\n",
            "5/20 loss 4.03\n",
            "5/20 loss 4.07\n",
            "5/20 loss 4.11\n",
            "5/20 loss 4.14\n",
            "5/20 loss 4.18\n",
            "5/20 loss 4.22\n",
            "5/20 loss 4.26\n",
            "5/20 loss 4.30\n",
            "5/20 loss 4.34\n",
            "10/20 loss 0.04\n",
            "10/20 loss 0.08\n",
            "10/20 loss 0.12\n",
            "10/20 loss 0.15\n",
            "10/20 loss 0.19\n",
            "10/20 loss 0.21\n",
            "10/20 loss 0.25\n",
            "10/20 loss 0.28\n",
            "10/20 loss 0.31\n",
            "10/20 loss 0.34\n",
            "10/20 loss 0.37\n",
            "10/20 loss 0.41\n",
            "10/20 loss 0.45\n",
            "10/20 loss 0.50\n",
            "10/20 loss 0.54\n",
            "10/20 loss 0.58\n",
            "10/20 loss 0.62\n",
            "10/20 loss 0.66\n",
            "10/20 loss 0.70\n",
            "10/20 loss 0.74\n",
            "10/20 loss 0.77\n",
            "10/20 loss 0.80\n",
            "10/20 loss 0.83\n",
            "10/20 loss 0.87\n",
            "10/20 loss 0.91\n",
            "10/20 loss 0.95\n",
            "10/20 loss 0.98\n",
            "10/20 loss 1.03\n",
            "10/20 loss 1.07\n",
            "10/20 loss 1.11\n",
            "10/20 loss 1.15\n",
            "10/20 loss 1.18\n",
            "10/20 loss 1.22\n",
            "10/20 loss 1.26\n",
            "10/20 loss 1.31\n",
            "10/20 loss 1.34\n",
            "10/20 loss 1.37\n",
            "10/20 loss 1.41\n",
            "10/20 loss 1.45\n",
            "10/20 loss 1.49\n",
            "10/20 loss 1.52\n",
            "10/20 loss 1.56\n",
            "10/20 loss 1.59\n",
            "10/20 loss 1.63\n",
            "10/20 loss 1.67\n",
            "10/20 loss 1.71\n",
            "10/20 loss 1.75\n",
            "10/20 loss 1.79\n",
            "10/20 loss 1.83\n",
            "10/20 loss 1.86\n",
            "10/20 loss 1.89\n",
            "10/20 loss 1.93\n",
            "10/20 loss 1.96\n",
            "10/20 loss 1.99\n",
            "10/20 loss 2.03\n",
            "10/20 loss 2.07\n",
            "10/20 loss 2.11\n",
            "10/20 loss 2.15\n",
            "10/20 loss 2.19\n",
            "10/20 loss 2.23\n",
            "10/20 loss 2.27\n",
            "10/20 loss 2.31\n",
            "10/20 loss 2.35\n",
            "10/20 loss 2.38\n",
            "10/20 loss 2.42\n",
            "10/20 loss 2.46\n",
            "10/20 loss 2.48\n",
            "10/20 loss 2.52\n",
            "10/20 loss 2.55\n",
            "10/20 loss 2.59\n",
            "10/20 loss 2.61\n",
            "10/20 loss 2.64\n",
            "10/20 loss 2.67\n",
            "10/20 loss 2.72\n",
            "10/20 loss 2.76\n",
            "10/20 loss 2.80\n",
            "10/20 loss 2.84\n",
            "10/20 loss 2.86\n",
            "10/20 loss 2.90\n",
            "10/20 loss 2.94\n",
            "10/20 loss 2.97\n",
            "10/20 loss 3.01\n",
            "10/20 loss 3.04\n",
            "10/20 loss 3.08\n",
            "10/20 loss 3.12\n",
            "10/20 loss 3.16\n",
            "10/20 loss 3.20\n",
            "10/20 loss 3.23\n",
            "10/20 loss 3.26\n",
            "10/20 loss 3.28\n",
            "10/20 loss 3.33\n",
            "10/20 loss 3.36\n",
            "10/20 loss 3.40\n",
            "10/20 loss 3.45\n",
            "10/20 loss 3.48\n",
            "10/20 loss 3.52\n",
            "10/20 loss 3.56\n",
            "10/20 loss 3.59\n",
            "10/20 loss 3.63\n",
            "10/20 loss 3.66\n",
            "10/20 loss 3.68\n",
            "10/20 loss 3.72\n",
            "10/20 loss 3.76\n",
            "10/20 loss 3.79\n",
            "10/20 loss 3.83\n",
            "10/20 loss 3.85\n",
            "10/20 loss 3.89\n",
            "10/20 loss 3.92\n",
            "10/20 loss 3.96\n",
            "10/20 loss 3.98\n",
            "10/20 loss 4.02\n",
            "15/20 loss 0.03\n",
            "15/20 loss 0.07\n",
            "15/20 loss 0.11\n",
            "15/20 loss 0.14\n",
            "15/20 loss 0.18\n",
            "15/20 loss 0.19\n",
            "15/20 loss 0.23\n",
            "15/20 loss 0.26\n",
            "15/20 loss 0.29\n",
            "15/20 loss 0.32\n",
            "15/20 loss 0.34\n",
            "15/20 loss 0.38\n",
            "15/20 loss 0.42\n",
            "15/20 loss 0.46\n",
            "15/20 loss 0.50\n",
            "15/20 loss 0.54\n",
            "15/20 loss 0.58\n",
            "15/20 loss 0.62\n",
            "15/20 loss 0.66\n",
            "15/20 loss 0.70\n",
            "15/20 loss 0.73\n",
            "15/20 loss 0.75\n",
            "15/20 loss 0.78\n",
            "15/20 loss 0.82\n",
            "15/20 loss 0.85\n",
            "15/20 loss 0.89\n",
            "15/20 loss 0.93\n",
            "15/20 loss 0.97\n",
            "15/20 loss 1.01\n",
            "15/20 loss 1.04\n",
            "15/20 loss 1.08\n",
            "15/20 loss 1.12\n",
            "15/20 loss 1.15\n",
            "15/20 loss 1.19\n",
            "15/20 loss 1.23\n",
            "15/20 loss 1.26\n",
            "15/20 loss 1.29\n",
            "15/20 loss 1.33\n",
            "15/20 loss 1.37\n",
            "15/20 loss 1.40\n",
            "15/20 loss 1.44\n",
            "15/20 loss 1.47\n",
            "15/20 loss 1.50\n",
            "15/20 loss 1.54\n",
            "15/20 loss 1.58\n",
            "15/20 loss 1.62\n",
            "15/20 loss 1.66\n",
            "15/20 loss 1.69\n",
            "15/20 loss 1.73\n",
            "15/20 loss 1.75\n",
            "15/20 loss 1.78\n",
            "15/20 loss 1.82\n",
            "15/20 loss 1.85\n",
            "15/20 loss 1.87\n",
            "15/20 loss 1.91\n",
            "15/20 loss 1.94\n",
            "15/20 loss 1.98\n",
            "15/20 loss 2.02\n",
            "15/20 loss 2.06\n",
            "15/20 loss 2.10\n",
            "15/20 loss 2.13\n",
            "15/20 loss 2.17\n",
            "15/20 loss 2.21\n",
            "15/20 loss 2.24\n",
            "15/20 loss 2.27\n",
            "15/20 loss 2.31\n",
            "15/20 loss 2.33\n",
            "15/20 loss 2.36\n",
            "15/20 loss 2.40\n",
            "15/20 loss 2.43\n",
            "15/20 loss 2.45\n",
            "15/20 loss 2.47\n",
            "15/20 loss 2.50\n",
            "15/20 loss 2.54\n",
            "15/20 loss 2.58\n",
            "15/20 loss 2.62\n",
            "15/20 loss 2.65\n",
            "15/20 loss 2.68\n",
            "15/20 loss 2.72\n",
            "15/20 loss 2.75\n",
            "15/20 loss 2.78\n",
            "15/20 loss 2.82\n",
            "15/20 loss 2.84\n",
            "15/20 loss 2.88\n",
            "15/20 loss 2.92\n",
            "15/20 loss 2.96\n",
            "15/20 loss 2.99\n",
            "15/20 loss 3.02\n",
            "15/20 loss 3.05\n",
            "15/20 loss 3.07\n",
            "15/20 loss 3.11\n",
            "15/20 loss 3.15\n",
            "15/20 loss 3.18\n",
            "15/20 loss 3.22\n",
            "15/20 loss 3.25\n",
            "15/20 loss 3.29\n",
            "15/20 loss 3.32\n",
            "15/20 loss 3.36\n",
            "15/20 loss 3.39\n",
            "15/20 loss 3.42\n",
            "15/20 loss 3.44\n",
            "15/20 loss 3.47\n",
            "15/20 loss 3.51\n",
            "15/20 loss 3.54\n",
            "15/20 loss 3.58\n",
            "15/20 loss 3.59\n",
            "15/20 loss 3.62\n",
            "15/20 loss 3.65\n",
            "15/20 loss 3.68\n",
            "15/20 loss 3.70\n",
            "15/20 loss 3.74\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CBOW(\n",
              "  (embeddings): Embedding(97, 10)\n",
              "  (linear1): Linear(in_features=10, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=97, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "Qgjydm-BaSCm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = CBOW(vocab_size,EMBEDDING_DIM)\n",
        "\n",
        "def test_cbow(model , vocab , word_to_ix) :\n",
        "    word_1 = vocab[10]\n",
        "    word_2 = vocab[5]\n",
        "    \n",
        "    word_1_vec = model.get_word_vector(word_1,word_to_ix)\n",
        "    word_2_vec = model.get_word_vector(word_2,word_to_ix)\n",
        "    \n",
        "    cosine_similarity = (torch.mm(word_1_vec, word_2_vec.transpose(0,1))) / (torch.norm(word_1_vec) * torch.norm(word_2_vec))\n",
        "    similarity = cosine_similarity.data.numpy()[0][0]\n",
        "    print('word1 : ',word_1)\n",
        "    print('word2 : ',word_2)\n",
        "    print('similarity : ',similarity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sGhiAUSJaSCp",
        "colab_type": "code",
        "outputId": "41652cfc-d1de-448a-c3b8-d8fb08e9b0fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "test_cbow(model,vocab,word_to_ix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word1 :  by\n",
            "word2 :  gazed\n",
            "similarity :  0.31032938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hBinXaEreki_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}