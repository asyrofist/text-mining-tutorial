{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9_Naive Bayesian Classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fininsight/text-mining-tutorial/blob/master/9_Naive_Bayesian_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSHRKud41Tr0",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayese Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypuyUtI4LAQs",
        "colab_type": "code",
        "outputId": "f786dfa9-51c3-4151-c46b-6f44ea0b89aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "k =0.5\n",
        "\n",
        "input_file = pd.read_csv(\"spam.csv\")\n",
        "training_set = np.array(input_file)\n",
        "\n",
        "# 범주에 속하는 문서수 세기 1(예. 긍정), 0(예. 부정))\n",
        "doccnt1 = 0\n",
        "doccnt0 = 0\n",
        "    \n",
        "# 토큰별로 문서내 빈도수 카운팅\n",
        "wordfreq = defaultdict(lambda : [0, 0])\n",
        "for doc, point in training_set:\n",
        "      words = doc.split()\n",
        "      for word in words :\n",
        "        if point == 1 :\n",
        "          wordfreq[word][0] += 1\n",
        "        else :\n",
        "          wordfreq[word][1] += 1\n",
        "          \n",
        "for key, (cnt1, cnt0) in wordfreq.items() :\n",
        "  if cnt1 > 0 :\n",
        "    doccnt1 += 1\n",
        "  if cnt0 > 0 :\n",
        "    doccnt0 += 1\n",
        "    \n",
        "print(doccnt0)\n",
        "\n",
        "wordprobs = defaultdict(lambda : [0, 0])\n",
        "for key, (cnt1, cnt0) in wordfreq.items() :\n",
        "  wordprobs[key][0] = (cnt1 + k) / (doccnt1 + 2*k)\n",
        "  wordprobs[key][1] = (cnt0 + k) / (doccnt0 + 2*k)\n",
        "\n",
        "print(len(word_probs))\n",
        "\n",
        "doc = \"free lottery\"\n",
        "tokens = doc.split()\n",
        "\n",
        "# 초기값은 모두 0으로 처리\n",
        "log_prob1 = log_prob0 = 0.0\n",
        "\n",
        "# 모든 단어에 대해 반복\n",
        "for word, (prob1, prob0) in wordprobs.items():\n",
        "    if word in tokens:\n",
        "        log_prob1 += math.log(prob1)\n",
        "        log_prob0 += math.log(prob0)\n",
        "        \n",
        "    else:\n",
        "        log_prob1 += math.log(1.0 - prob1)\n",
        "        log_prob0 += math.log(1.0 - prob0)\n",
        "\n",
        "    \n",
        "log_prob1 += math.log(doccnt1/len(wordfreq))\n",
        "log_prob0 += math.log(doccnt0/len(wordfreq))\n",
        "  \n",
        "prob1 = math.exp(log_prob1)\n",
        "prob0 = math.exp(log_prob0)\n",
        "print(prob1 / (prob1 + prob0))\n",
        "print(prob0 / (prob1 + prob0))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "17\n",
            "0.8964758762435877\n",
            "0.10352412375641222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdNBH_cptl1u",
        "colab_type": "text"
      },
      "source": [
        "P(wi|positive)=k+count(wi,positive)2k+∑w∈Vcount(w,positive)\n"
      ]
    }
  ]
}