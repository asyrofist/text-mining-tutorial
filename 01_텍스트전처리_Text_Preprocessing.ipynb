{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0_텍스트전처리_Text Preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fininsight/text-mining-tutorial/blob/master/0_%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%A0%84%EC%B2%98%EB%A6%AC_Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLGhbEiOoAR7",
        "colab_type": "text"
      },
      "source": [
        "# 텍스트 전처리 (Text Preprocessing)\n",
        "\n",
        "*   텍스트를 자연어 처리를 위해 용도에 맞도록 사전에 표준화 하는 작업\n",
        "*   텍스트 내 정보를 유지하고, 중복을 제거하여 분석 효율성을 높이기 위해 전처리를 수행\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E585k45HDx5E",
        "colab_type": "text"
      },
      "source": [
        "### 1) 토큰화 (Tokenizing)\n",
        "* 텍스트를 자연어 처리를 위해 분리 하는 것을\n",
        "* 토큰화는 단어별로 분리하는 \"단어 토큰화(Word Tokenization)\"와 문장별로 분리하는 \"문장 토큰화(Sentence Tokenization)\"로 구분\n",
        "\n",
        "(이후 실습에서는 단어 토큰화를 \"토큰화\"로 통일하여 칭하도록 한다)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "senwNSwgDzQc",
        "colab_type": "text"
      },
      "source": [
        "### 2) 품사 부착(PoS Tagging)\n",
        "* 각 토큰에 품사 정보를 추가\n",
        "* 분석시에 불필요한 품사를 제거하거나 (예. 조사, 접속사 등) 필요한 품사를 필터링 하기 위해 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R15ri5czDyzc",
        "colab_type": "text"
      },
      "source": [
        "### 3) 개체명 인식 (NER, Named Entity Recognition)\n",
        "* 각 토큰의 개체 구분(기관, 인물, 지역, 날짜 등) 태그를 부착\n",
        "* 텍스트가 무엇과 관련되어있는지 구분하기 위해 사용\n",
        "* 예를 들어, 과일의 apple과 기업의 apple을 구분하는 방법이 개체명 인식임"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfq99EkzD1Tk",
        "colab_type": "text"
      },
      "source": [
        "### 4) 원형 복원 (Stemming & Lemmatization)\n",
        "* 각 토큰의 원형 복원을 함으로써 토큰을 표준화하여 불필요한 데이터 중복을 방지 (=단어의 수를 줄일수 있어 연산을 효율성을 높임)\n",
        "* 어간 추출(Stemming) : 품사를 무시하고 규칙에 기반하여 어간을 추출\n",
        "* 표제어 추출 (Lemmatization) : 품사정보를 유지하여 표제어 추출"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5HQOjRvDxmd",
        "colab_type": "text"
      },
      "source": [
        "### 5) 불용어 처리 (Stopword)\n",
        "* 자연어 처리를 위해 불필요한 요소를 제거하는 작업\n",
        "* 불필요한 품사를 제거하는 작업과 불필요한 단어를 제거하는 작업으로 구성\n",
        "* 불필요한 토큰을 제거함으로써 연산의 효율성을 높임"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaIYJczuaS0n",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KysKAL3VlgQN",
        "colab_type": "text"
      },
      "source": [
        "# 1 영문 전처리 실습\n",
        "\n",
        "\n",
        "NLTK lib (https://www.nltk.org/) 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mND0us3Jppcu",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 실습용 영문기사 수집\n",
        "온라인 기사를 바로 수집하여 실습데이터로 사용\n",
        "\n",
        "https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4pcegjEqewF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFUHpRU3qhLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/?ss=ai-big-data#45dd5dd61f25'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text,'html.parser')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlX5zJweqn-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eng_news = soup.select('p') #[class=\"speakable-paragraph\"]')\n",
        "eng_text = eng_news[3].get_text()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urFWZplwq5My",
        "colab_type": "code",
        "outputId": "85d13cba-1c34-4e3e-b130-2d91f35d8c25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "eng_text"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"And yes, she does mean everybody's job from yours to mine and onward to the role of grain farmers in Egypt, pastry chefs in Paris and dog walkers in Oregon i.e. every job. We will now be able to help direct all workers’ actions and behavior with a new degree of intelligence that comes from predictive analytics, all stemming from the AI engines we will now increasingly depend upon.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv0ASXb8qa6H",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 영문 토큰화\n",
        "https://www.nltk.org/api/nltk.tokenize.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywTmZDer4iH-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "239e6c2d-54e6-4d4b-cf2c-fbb6eb6f0eb1"
      },
      "source": [
        "# word_tokenize() : 마침표와 구두점(온점(.), 컴마(,), 물음표(?), 세미콜론(;), 느낌표(!) 등과 같은 기호)으로 구분하여 토큰화\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "token1 = word_tokenize('Barack Obama likes fried chicken very much. New York')\n",
        "print(token1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['Barack', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much', '.', 'New', 'York']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ecl0DtolnEc",
        "colab_type": "code",
        "outputId": "f53d5c24-e599-426d-f6f8-647f9e812978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "# word_tokenize() : 마침표와 구두점(온점(.), 컴마(,), 물음표(?), 세미콜론(;), 느낌표(!) 등과 같은 기호)으로 구분하여 토큰화\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "token1 = word_tokenize(eng_text)\n",
        "print(token1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rygb4BNXFd13",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f0b6e142-41b8-46f6-bdd4-7691fbe25b89"
      },
      "source": [
        "# WordPunctTokenizer() : 알파벳과 알파벳이 아닌문자를 구분하여 토큰화\n",
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "token2 = WordPunctTokenizer().tokenize(eng_text)\n",
        "print(token2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'\", 's', 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i', '.', 'e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrvBRJqJlitx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "807e86b0-5e88-45dd-8794-f7155e5cfaeb"
      },
      "source": [
        "# TreebankWordTokenizer() : 정규표현식에 기반한 토큰화\n",
        "import nltk\n",
        "nltk.download('punkt') #punctuation 정보 다운로드\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "token = TreebankWordTokenizer().tokenize(eng_text)\n",
        "print(token)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e.', 'every', 'job.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-Z-0Nnysqnq",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 영문 품사 부착 (PoS Tagging)\n",
        "분리한 토큰마다 품사를 부착한다\n",
        "\n",
        "https://www.nltk.org/api/nltk.tag.html\n",
        "\n",
        "태크목록 : https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHWVrEmTlosg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "757220b7-b304-471e-a6fe-beec65f4ccb9"
      },
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFL2SbsyCHiR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "93608f14-1d1c-4dd6-b6a2-b894621a3f8f"
      },
      "source": [
        "taggedToken = pos_tag(token1)\n",
        "print(taggedToken)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Barack', 'NNP'), ('Obama', 'NNP'), ('likes', 'VBZ'), ('fried', 'VBN'), ('chicken', 'JJ'), ('very', 'RB'), ('much', 'RB'), ('.', '.'), ('New', 'NNP'), ('York', 'NNP')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwtt2LxqlrVS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "53bb66da-e608-4ac1-ab6d-4806edba93af"
      },
      "source": [
        "taggedToken = pos_tag(token)\n",
        "print(taggedToken)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('And', 'CC'), ('yes', 'UH'), (',', ','), ('she', 'PRP'), ('does', 'VBZ'), ('mean', 'VB'), ('everybody', 'NN'), (\"'s\", 'POS'), ('job', 'NN'), ('from', 'IN'), ('yours', 'NNS'), ('to', 'TO'), ('mine', 'VB'), ('and', 'CC'), ('onward', 'VB'), ('to', 'TO'), ('the', 'DT'), ('role', 'NN'), ('of', 'IN'), ('grain', 'NN'), ('farmers', 'NNS'), ('in', 'IN'), ('Egypt', 'NNP'), (',', ','), ('pastry', 'NN'), ('chefs', 'NNS'), ('in', 'IN'), ('Paris', 'NNP'), ('and', 'CC'), ('dog', 'NN'), ('walkers', 'NNS'), ('in', 'IN'), ('Oregon', 'NNP'), ('i.e.', 'NN'), ('every', 'DT'), ('job.', 'NN'), ('We', 'PRP'), ('will', 'MD'), ('now', 'RB'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('help', 'VB'), ('direct', 'VB'), ('all', 'DT'), ('workers', 'NNS'), ('’', 'VBP'), ('actions', 'NNS'), ('and', 'CC'), ('behavior', 'NN'), ('with', 'IN'), ('a', 'DT'), ('new', 'JJ'), ('degree', 'NN'), ('of', 'IN'), ('intelligence', 'NN'), ('that', 'WDT'), ('comes', 'VBZ'), ('from', 'IN'), ('predictive', 'JJ'), ('analytics', 'NNS'), (',', ','), ('all', 'DT'), ('stemming', 'VBG'), ('from', 'IN'), ('the', 'DT'), ('AI', 'NNP'), ('engines', 'VBZ'), ('we', 'PRP'), ('will', 'MD'), ('now', 'RB'), ('increasingly', 'RB'), ('depend', 'VBP'), ('upon', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDo-5-khs5Oz",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 개체명 인식 (NER, Named Entity Recognition)\n",
        "\n",
        "http://www.nltk.org/api/nltk.chunk.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clj4X6Gilsi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "63213f0f-394d-4b63-ad4a-32039f0ac9d7"
      },
      "source": [
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaM69WbLl_Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import ne_chunk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdkMJHO7mBgi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "ddc35aff-2740-4861-b770-3d7d12c0e188"
      },
      "source": [
        "neToken = ne_chunk(taggedToken)\n",
        "print(neToken)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (ORGANIZATION Obama/NNP)\n",
            "  likes/VBZ\n",
            "  fried/VBN\n",
            "  chicken/JJ\n",
            "  very/RB\n",
            "  much/RB\n",
            "  ./.\n",
            "  (GPE New/NNP York/NNP))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHjV0h0ZtM-t",
        "colab_type": "text"
      },
      "source": [
        "## 1.5 원형 복원\n",
        "각 토큰의 원형을 복원하여 표준화 한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2eCnbChtXjo",
        "colab_type": "text"
      },
      "source": [
        "### 1.5.1 어간추출 (Stemming)\n",
        "\n",
        "* 규칙에 기반 하여 토큰을 표준화\n",
        "* ning제거, ful 제거 등\n",
        "\n",
        "https://www.nltk.org/api/nltk.stem.html\n",
        "\n",
        "규칙상세 : https://tartarus.org/martin/PorterStemmer/def.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-AvZXHLmCy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "ffc718b7-d123-44e8-9027-1db99c0236c5"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "print(\"running -> \" + ps.stem(\"running\"))\n",
        "print(\"beautiful -> \" + ps.stem(\"beautiful\"))\n",
        "print(\"believes -> \" + ps.stem(\"believes\"))\n",
        "print(\"using -> \" + ps.stem(\"using\"))\n",
        "print(\"conversation -> \" + ps.stem(\"conversation\"))\n",
        "print(\"organization -> \" + ps.stem(\"organization\"))\n",
        "print(\"studies -> \" + ps.stem(\"studies\"))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running -> run\n",
            "beautiful -> beauti\n",
            "believes -> believ\n",
            "using -> use\n",
            "conversation -> convers\n",
            "organization -> organ\n",
            "studies -> studi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdxBuzdymR7w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "94bb714d-9b2d-414a-9b18-9097a0ef52df"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4haNWIcCtZza",
        "colab_type": "text"
      },
      "source": [
        "### 1.5.2 표제어 추출 (Lemmatization)\n",
        "\n",
        "* 품사정보를 보존하여 토큰을 표준화\n",
        "\n",
        "http://www.nltk.org/api/nltk.stem.html?highlight=lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mQSzsCZmMBd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "4687d9d4-3bad-4725-d8e2-8f6295cabcfd"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "print(\"running -> \" + wl.lemmatize(\"running\"))\n",
        "print(\"beautiful -> \" + wl.lemmatize(\"beautiful\"))\n",
        "print(\"believes -> \" + wl.lemmatize(\"believes\"))\n",
        "print(\"using -> \" + wl.lemmatize(\"using\"))\n",
        "print(\"conversation -> \" + wl.lemmatize(\"conversation\"))\n",
        "print(\"organization -> \" + wl.lemmatize(\"organization\"))\n",
        "print(\"studies -> \" + wl.lemmatize(\"studies\"))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running -> running\n",
            "beautiful -> beautiful\n",
            "believes -> belief\n",
            "using -> using\n",
            "conversation -> conversation\n",
            "organization -> organization\n",
            "studies -> study\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmY_SvDMb0fz",
        "colab_type": "text"
      },
      "source": [
        "## 1.6 불용어 처리 (Stopword)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOUE-BBKcn4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopPos = ['IN', 'CC', 'UH', 'TO', 'MD', 'DT', 'VBZ','VBP']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyDJ4JiscnrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1097
        },
        "outputId": "f8a2082a-0fff-40f8-b550-b406a21fa265"
      },
      "source": [
        "# 최빈어 조회. 최빈어를 조회하여 불용어 제거 대상을 선정\n",
        "from collections import Counter\n",
        "Counter(taggedToken).most_common()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((',', ','), 3),\n",
              " (('from', 'IN'), 3),\n",
              " (('to', 'TO'), 3),\n",
              " (('and', 'CC'), 3),\n",
              " (('in', 'IN'), 3),\n",
              " (('the', 'DT'), 2),\n",
              " (('of', 'IN'), 2),\n",
              " (('will', 'MD'), 2),\n",
              " (('now', 'RB'), 2),\n",
              " (('all', 'DT'), 2),\n",
              " (('And', 'CC'), 1),\n",
              " (('yes', 'UH'), 1),\n",
              " (('she', 'PRP'), 1),\n",
              " (('does', 'VBZ'), 1),\n",
              " (('mean', 'VB'), 1),\n",
              " (('everybody', 'NN'), 1),\n",
              " ((\"'s\", 'POS'), 1),\n",
              " (('job', 'NN'), 1),\n",
              " (('yours', 'NNS'), 1),\n",
              " (('mine', 'VB'), 1),\n",
              " (('onward', 'VB'), 1),\n",
              " (('role', 'NN'), 1),\n",
              " (('grain', 'NN'), 1),\n",
              " (('farmers', 'NNS'), 1),\n",
              " (('Egypt', 'NNP'), 1),\n",
              " (('pastry', 'NN'), 1),\n",
              " (('chefs', 'NNS'), 1),\n",
              " (('Paris', 'NNP'), 1),\n",
              " (('dog', 'NN'), 1),\n",
              " (('walkers', 'NNS'), 1),\n",
              " (('Oregon', 'NNP'), 1),\n",
              " (('i.e.', 'NN'), 1),\n",
              " (('every', 'DT'), 1),\n",
              " (('job.', 'NN'), 1),\n",
              " (('We', 'PRP'), 1),\n",
              " (('be', 'VB'), 1),\n",
              " (('able', 'JJ'), 1),\n",
              " (('help', 'VB'), 1),\n",
              " (('direct', 'VB'), 1),\n",
              " (('workers', 'NNS'), 1),\n",
              " (('’', 'VBP'), 1),\n",
              " (('actions', 'NNS'), 1),\n",
              " (('behavior', 'NN'), 1),\n",
              " (('with', 'IN'), 1),\n",
              " (('a', 'DT'), 1),\n",
              " (('new', 'JJ'), 1),\n",
              " (('degree', 'NN'), 1),\n",
              " (('intelligence', 'NN'), 1),\n",
              " (('that', 'WDT'), 1),\n",
              " (('comes', 'VBZ'), 1),\n",
              " (('predictive', 'JJ'), 1),\n",
              " (('analytics', 'NNS'), 1),\n",
              " (('stemming', 'VBG'), 1),\n",
              " (('AI', 'NNP'), 1),\n",
              " (('engines', 'VBZ'), 1),\n",
              " (('we', 'PRP'), 1),\n",
              " (('increasingly', 'RB'), 1),\n",
              " (('depend', 'VBP'), 1),\n",
              " (('upon', 'NN'), 1),\n",
              " (('.', '.'), 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNhxqDVkcnX9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "72c51202-5724-444c-b83e-784233c3f594"
      },
      "source": [
        "stopWord = [',','be','able']\n",
        "\n",
        "word = []\n",
        "for tag in taggedToken:\n",
        "    if tag[1] not in stopPos:\n",
        "        if tag[0] not in stopWord:\n",
        "            word.append(tag[0])\n",
        "            \n",
        "print(word)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['she', 'mean', 'everybody', \"'s\", 'job', 'yours', 'mine', 'onward', 'role', 'grain', 'farmers', 'Egypt', 'pastry', 'chefs', 'Paris', 'dog', 'walkers', 'Oregon', 'i.e.', 'job.', 'We', 'now', 'help', 'direct', 'workers', 'actions', 'behavior', 'new', 'degree', 'intelligence', 'that', 'predictive', 'analytics', 'stemming', 'AI', 'we', 'now', 'increasingly', 'upon', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV0orUsOb6wD",
        "colab_type": "text"
      },
      "source": [
        "## 1.7 영문 텍스트 전처리 종합"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbz6tLP_mNrn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1729
        },
        "outputId": "efb597ac-5a2e-465d-c9e2-47e6c863f724"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "sumtoken = TreebankWordTokenizer().tokenize(\"Obama loves fried chicken of KFC\")\n",
        "print(token)\n",
        "\n",
        "from nltk import pos_tag\n",
        "sumTaggedToken = pos_tag(sumtoken)\n",
        "print(taggedToken)\n",
        "\n",
        "from nltk import ne_chunk\n",
        "sumNeToken = ne_chunk(sumTaggedToken)\n",
        "print(neToken)\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "print(\"loves -> \" + ps.stem(\"loves\"))\n",
        "print(\"fried -> \" + ps.stem(\"fried\"))\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "print(\"loves -> \" + wl.lemmatize(\"loves\"))\n",
        "print(\"fried -> \" + wl.lemmatize(\"fried\"))\n",
        "\n",
        "#불용어 처리\n",
        "sumStopPos = ['IN']\n",
        "sumStopWord = ['fried']\n",
        "\n",
        "word = []\n",
        "for tag in sumTaggedToken:\n",
        "    if tag[1] not in sumStopPos:\n",
        "        if tag[0] not in sumStopWord:\n",
        "            word.append(tag[0])\n",
        "            \n",
        "print(word)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e.', 'every', 'job.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n",
            "[('And', 'CC'), ('yes', 'UH'), (',', ','), ('she', 'PRP'), ('does', 'VBZ'), ('mean', 'VB'), ('everybody', 'NN'), (\"'s\", 'POS'), ('job', 'NN'), ('from', 'IN'), ('yours', 'NNS'), ('to', 'TO'), ('mine', 'VB'), ('and', 'CC'), ('onward', 'VB'), ('to', 'TO'), ('the', 'DT'), ('role', 'NN'), ('of', 'IN'), ('grain', 'NN'), ('farmers', 'NNS'), ('in', 'IN'), ('Egypt', 'NNP'), (',', ','), ('pastry', 'NN'), ('chefs', 'NNS'), ('in', 'IN'), ('Paris', 'NNP'), ('and', 'CC'), ('dog', 'NN'), ('walkers', 'NNS'), ('in', 'IN'), ('Oregon', 'NNP'), ('i.e.', 'NN'), ('every', 'DT'), ('job.', 'NN'), ('We', 'PRP'), ('will', 'MD'), ('now', 'RB'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('help', 'VB'), ('direct', 'VB'), ('all', 'DT'), ('workers', 'NNS'), ('’', 'VBP'), ('actions', 'NNS'), ('and', 'CC'), ('behavior', 'NN'), ('with', 'IN'), ('a', 'DT'), ('new', 'JJ'), ('degree', 'NN'), ('of', 'IN'), ('intelligence', 'NN'), ('that', 'WDT'), ('comes', 'VBZ'), ('from', 'IN'), ('predictive', 'JJ'), ('analytics', 'NNS'), (',', ','), ('all', 'DT'), ('stemming', 'VBG'), ('from', 'IN'), ('the', 'DT'), ('AI', 'NNP'), ('engines', 'VBZ'), ('we', 'PRP'), ('will', 'MD'), ('now', 'RB'), ('increasingly', 'RB'), ('depend', 'VBP'), ('upon', 'NN'), ('.', '.')]\n",
            "(S\n",
            "  And/CC\n",
            "  yes/UH\n",
            "  ,/,\n",
            "  she/PRP\n",
            "  does/VBZ\n",
            "  mean/VB\n",
            "  everybody/NN\n",
            "  's/POS\n",
            "  job/NN\n",
            "  from/IN\n",
            "  yours/NNS\n",
            "  to/TO\n",
            "  mine/VB\n",
            "  and/CC\n",
            "  onward/VB\n",
            "  to/TO\n",
            "  the/DT\n",
            "  role/NN\n",
            "  of/IN\n",
            "  grain/NN\n",
            "  farmers/NNS\n",
            "  in/IN\n",
            "  (GPE Egypt/NNP)\n",
            "  ,/,\n",
            "  pastry/NN\n",
            "  chefs/NNS\n",
            "  in/IN\n",
            "  (GPE Paris/NNP)\n",
            "  and/CC\n",
            "  dog/NN\n",
            "  walkers/NNS\n",
            "  in/IN\n",
            "  (GPE Oregon/NNP)\n",
            "  i.e./NN\n",
            "  every/DT\n",
            "  job./NN\n",
            "  We/PRP\n",
            "  will/MD\n",
            "  now/RB\n",
            "  be/VB\n",
            "  able/JJ\n",
            "  to/TO\n",
            "  help/VB\n",
            "  direct/VB\n",
            "  all/DT\n",
            "  workers/NNS\n",
            "  ’/VBP\n",
            "  actions/NNS\n",
            "  and/CC\n",
            "  behavior/NN\n",
            "  with/IN\n",
            "  a/DT\n",
            "  new/JJ\n",
            "  degree/NN\n",
            "  of/IN\n",
            "  intelligence/NN\n",
            "  that/WDT\n",
            "  comes/VBZ\n",
            "  from/IN\n",
            "  predictive/JJ\n",
            "  analytics/NNS\n",
            "  ,/,\n",
            "  all/DT\n",
            "  stemming/VBG\n",
            "  from/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION AI/NNP)\n",
            "  engines/VBZ\n",
            "  we/PRP\n",
            "  will/MD\n",
            "  now/RB\n",
            "  increasingly/RB\n",
            "  depend/VBP\n",
            "  upon/NN\n",
            "  ./.)\n",
            "loves -> love\n",
            "fried -> fri\n",
            "loves -> love\n",
            "fried -> fried\n",
            "['Obama', 'loves', 'chicken', 'KFC']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMErzPcbuYEa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Dhqm4zkHXl",
        "colab_type": "text"
      },
      "source": [
        "# 2 한글 전처리 실습\n",
        "영문은 공백으로 토큰화가 가능하지만, 한글의 경우 품사를 고려하여 토큰화 해야한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIkGxDnNimek",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 실습용 한글기사 수집\n",
        "온라인 기사를 바로 수집하여 실습데이터로 사용\n",
        "\n",
        "http://news.chosun.com/site/data/html_dir/2018/07/10/2018071004121.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0hNPfAwhxwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imh_qjKTitdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'http://news.chosun.com/site/data/html_dir/2018/07/10/2018071004121.html'\n",
        "response = requests.get(url)\n",
        "response.encoding = 'utf-8'    # 한글이므로 encoding을 utf-8로 지정\n",
        "soup = BeautifulSoup(response.text,'html.parser')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2vMw982itqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kor_news = soup.select('div[class=\"par\"]')\n",
        "kor_text = kor_news[0].get_text()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7Cl3zwiitxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kor_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w09FHRgIphw5",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 한글 토큰화 및 형태소 분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj3gdRSzhC8n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "f29ea21e-697d-472e-9e80-039939d7df28"
      },
      "source": [
        "#konlpy 설치\n",
        "!pip install konlpy"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/3d/4e983cd98d87b50b2ab0387d73fa946f745aa8164e8888a714d5129f9765/konlpy-0.5.1-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 4.9MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.5.7 (from konlpy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/4b/60a3e63d51714d4d7ef1b1efdf84315d118a0a80a5b085bb52a7e2428cdc/JPype1-0.6.3.tar.gz (168kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 23.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: JPype1\n",
            "  Building wheel for JPype1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/2b/e8/c0b818ac4b3d35104d35e48cdc7afe27fc06ea277feed2831a\n",
            "Successfully built JPype1\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-0.6.3 konlpy-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP9Zj-WxwAtE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#출력을 위해서 임시 class \n",
        "class List(list): \n",
        "    def __str__(self): \n",
        "        return \"[\" + \", \".join([\"%s\" % x for x in self]) + \"]\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IZWN4xX4HXW",
        "colab_type": "text"
      },
      "source": [
        "한글 자연어처리기 비교\n",
        "\n",
        "https://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221337575742"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6AVbLfR632a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "38ff585c-1a28-49ac-a7f5-701ddf96e6a8"
      },
      "source": [
        "# Okt 토큰화\n",
        "from konlpy.tag import Okt\n",
        "okt= Okt()\n",
        "okt_tokens = okt.morphs('버락 오바마는 후라이드 치킨을 너무 좋아한다.')\n",
        "print(List(okt_tokens))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[버락, 오바마, 는, 후라이드, 치킨, 을, 너무, 좋아한다, .]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__e0d_9Svzor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 코모란(Komoran) 토큰화\n",
        "from konlpy.tag import Komoran\n",
        "komoran= Komoran()\n",
        "komoran_tokens = komoran.morphs(kor_text)\n",
        "print(List(komoran_tokens))\n",
        "\n",
        "# 한나눔(Hannanum) 토큰화\n",
        "from konlpy.tag import Hannanum\n",
        "hannanum= Hannanum()\n",
        "hannanum_tokens = hannanum.morphs(kor_text)\n",
        "print(List(hannanum_tokens))\n",
        "\n",
        "# Okt 토큰화\n",
        "from konlpy.tag import Okt\n",
        "okt= Okt()\n",
        "okt_tokens = okt.morphs(kor_text)\n",
        "print(List(okt_tokens))\n",
        "\n",
        "# Kkma 토큰화\n",
        "from konlpy.tag import Kkma\n",
        "kkma= Kkma()\n",
        "kkma_tokens = kkma.morphs(kor_text)\n",
        "print(List(kkma_tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M7nyptjunTG",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 한글 품사 부착 (PoS Tagging)\n",
        "\n",
        "PoS Tag 목록\n",
        "\n",
        "https://docs.google.com/spreadsheets/u/1/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t6txrctj8nC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 코모란(Komoran) 품사 태깅\n",
        "komoranTag = []\n",
        "for token in komoran_tokens:\n",
        "    komoranTag += komoran.pos(token)\n",
        "print(komoranTag[:10])\n",
        "\n",
        "# 한나눔(Hannanum) 품사 태깅\n",
        "hannanumTag = []\n",
        "for token in hannanum_tokens:\n",
        "    hannanumTag += hannanum.pos(token)\n",
        "print(hannanumTag[:10])\n",
        "\n",
        "# Okt 품사 태깅\n",
        "oktTag = []\n",
        "for token in okt_tokens:\n",
        "    oktTag += okt.pos(token)\n",
        "print(oktTag[:10])\n",
        "\n",
        "# Kkma 품사 태깅\n",
        "kkmaTag = []\n",
        "for token in kkma_tokens:\n",
        "    kkmaTag += kkma.pos(token)\n",
        "print(kkmaTag[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZY4s8tbuuXP",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 불용어(Stopword) 처리\n",
        "분석에 불필요한 품사를 제거하고, 불필요한 단어(불용어)를 제거한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvjk1yIYkCfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#불용어 처리\n",
        "stopPos = ['Suffix','Punctuation','Josa','Foreign','Alpha','Number']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "573iqrTFkcJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 최빈어 조회. 최빈어를 조회하여 불용어 제거 대상을 선정\n",
        "from collections import Counter\n",
        "Counter(oktTag).most_common()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lBkhHm1kYcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopWord = ['의','이','를','은','과','수','했다','것','있는','한다','하는','그','있다','할','이런','되기','해야','있게','여기']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJgERpoikh9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = []\n",
        "for tag in oktTag:\n",
        "    if tag[1] not in stopPos:\n",
        "        if tag[0] not in stopWord:\n",
        "            word.append(tag[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUQTDj4KkkBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(word)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}