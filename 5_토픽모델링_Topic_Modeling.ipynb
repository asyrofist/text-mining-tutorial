{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_토픽모델링_Topic Modeling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fininsight/text-mining-tutorial/blob/master/5_%ED%86%A0%ED%94%BD%EB%AA%A8%EB%8D%B8%EB%A7%81_Topic_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyp5nHXQsXez",
        "colab_type": "text"
      },
      "source": [
        "# 1 잠재의미분석 (LSA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0Ah5QgSqEce",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 실습 템플릿"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dsdajm39w73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSA :\n",
        "  def __init__(self, doc_ls, topic_num):\n",
        "    pass\n",
        "  \n",
        "  # tdm matrix 생성\n",
        "  def TDM(self, doc_ls):\n",
        "    pass\n",
        "  \n",
        "  # tdm matrix 특이값 분해(SVD)\n",
        "  # U, s, Vt로 분해\n",
        "  def SVD(self, tdm):\n",
        "    pass\n",
        "  \n",
        "  # 토픽별 주요 키워드 출력\n",
        "  def TopicModeling(self) :\n",
        "    pass\n",
        "  \n",
        "  # 단어 벡터 행렬 생성 dot(U,s)  \n",
        "  def TermVectorMatrix(self, u, s):\n",
        "    pass\n",
        "  \n",
        "  # 문서 벡터 행렬 생성 dot(s,Vt).T \n",
        "  def DocVectorMatrix(self, s, vt):\n",
        "    pass\n",
        "  \n",
        "  # 키워드를 입력했을 때 단어 벡터 반환\n",
        "  def GetTermVector(self, term):\n",
        "    pass\n",
        "  \n",
        "  # 문서를 입력했을 때 문서 벡터 반환\n",
        "  def GetDocVector(self, doc):\n",
        "    pass\n",
        "  \n",
        "  # 단어-문서 벡터 행렬 생성\n",
        "  def TermDocVectorMatrix(self, u, s, vt):\n",
        "    pass\n",
        "  \n",
        "  # 단어 벡터 행렬에서 단어 간 코사인 유사도 측정하여 행렬형태로 반환\n",
        "  def TermSimilarityMatrix(self, term_vec_matrix):\n",
        "    pass\n",
        "  \n",
        "  # 두개 단어를 입력했을 때 코사인 유사도 반환\n",
        "  def GetTermSimilarity(self, term1, term2):\n",
        "    pass\n",
        "  \n",
        "  # 문서 벡터 행렬에서 문서 간 코사인 유사도 측정하여 행렬형태로 반환\n",
        "  def DocSimilarityMartrix(self, doc_vec_matrix):\n",
        "    pass\n",
        "  \n",
        "  # 두개 문서를 입력했을 때 코사인 유사도 반환\n",
        "  def GetDocSimilarity(self, doc1, doc2):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHB9A2VjGE0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_ls = ['바나나 사과 포도 포도',\n",
        "         '사과 포도',\n",
        "         '포도 바나나',\n",
        "         '짜장면 짬뽕 탕수욕',\n",
        "         '볶음밥 탕수욕',\n",
        "         '짜장면 짬뽕',\n",
        "         '라면 스시',\n",
        "         '스시',\n",
        "         '가츠동 스시 소바',\n",
        "         '된장찌개 김치찌개 김치',\n",
        "         '김치 된장',\n",
        "         '비빔밥 김치'\n",
        "         ]\n",
        "\n",
        "lsa = LSA(doc_ls, 3)\n",
        "lsa.TopicModeling()\n",
        "lsa.GetTermSimilarity('사과','바나나')\n",
        "lsa.GetTermSimilarity('사과','짜장면')\n",
        "lsa.GetDocSimilarity('사과 포도', '포도 바나나')\n",
        "lsa.GetDocSimilarity('사과 포도', '라면 스시')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAQyWNVQWAh8",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 실습 예제 코드 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFgOmDD84TkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import randomized_svd\n",
        "\n",
        "class LSA :\n",
        "  def __init__(self, doc_ls, topic_num):\n",
        "    self.doc_ls = doc_ls\n",
        "    self.topic_num = topic_num\n",
        "    self.term2idx, self.idx2term = self.toIdxDict(' '.join(doc_ls).split())\n",
        "    self.doc2idx, self.idx2doc = self.toIdxDict(doc_ls)\n",
        "    \n",
        "    self.tdm = self.TDM(doc_ls)\n",
        "    self.U, self.s, self.VT = self.SVD(self.tdm)\n",
        "    \n",
        "    #print(self.s)\n",
        "    #print(self.U[:,:1].round(2))\n",
        "    \n",
        "    self.term_mat = self.TermVectorMatrix(self.U, self.s, topic_num)\n",
        "    self.doc_mat = self.DocVectorMatrix(self.s, self.VT, topic_num)\n",
        "    self.term_doc_mat = self.TermDocVectorMatrix(self.U, self.s, self.VT, topic_num)\n",
        "    \n",
        "    #print(self.term2idx.keys())\n",
        "    #print(self.term_mat.round(2))\n",
        "    \n",
        "    self.term_sim = self.TermSimilarityMatrix(self.term_mat)\n",
        "    self.doc_sim = self.DocSimilarityMartrix(self.doc_mat)\n",
        "    \n",
        "  # 리스트내 값을 index로 변환하는 dict과 \n",
        "  # index를 리스트내 값으로 변환하는 dict\n",
        "  def toIdxDict(self, ls) :\n",
        "    any2idx = defaultdict(lambda : len(any2idx))\n",
        "    idx2any = defaultdict()\n",
        "\n",
        "    for item in ls:\n",
        "        any2idx[item]\n",
        "        idx2any[any2idx[item]] = item\n",
        "        \n",
        "    return any2idx, idx2any\n",
        "  \n",
        "  def TDM(self, doc_ls):\n",
        "    # 행(토큰크기), 열(문서갯수)로 TDM 생성\n",
        "    tdm = np.zeros([len(self.term2idx.keys()), len(doc_ls)])\n",
        "    \n",
        "    for doc_idx, doc in enumerate(doc_ls) :\n",
        "      for term in doc.split() :\n",
        "        #등장한 단어를 dictionary에서 위치를 탐색하여 빈도수 세기\n",
        "        tdm[self.term2idx[term], doc_idx] += 1\n",
        "    \n",
        "    return tdm\n",
        "  \n",
        "  # 특이값 분해\n",
        "  def SVD(self, tdm):\n",
        "    U, s, VT = randomized_svd(X, \n",
        "                              n_components=15,\n",
        "                              n_iter=5,\n",
        "                              random_state=None)\n",
        "    \n",
        "    #U, s, VT = np.linalg.svd(tdm)\n",
        "    return U, s, VT\n",
        "  \n",
        "  # 토픽별 주요 키워드 출력\n",
        "  def TopicModeling(self, count = 3) :\n",
        "    topic_num = self.topic_num\n",
        "    \n",
        "    for i in range(topic_num) :\n",
        "      score = self.U[:,i:i+1].T\n",
        "      sorted_index = np.flip(np.argsort(-score),0)\n",
        "      \n",
        "      a = []\n",
        "      for j in sorted_index[0,: count] :\n",
        "        a.append((self.idx2term[j], score[0,j].round(3)))\n",
        "      \n",
        "      print(\"Topic {} - {}\".format(i+1,a ))\n",
        "  \n",
        "  def vectorSimilarity(self, matrix) :\n",
        "    similarity = np.zeros([matrix.shape[1], matrix.shape[1]])\n",
        "    \n",
        "    for i in range(matrix.shape[1]) :\n",
        "      for j in range(matrix.shape[1]) :\n",
        "        similarity[i,j] =  cosine_similarity(matrix[:,i].T, matrix[:,j].T)\n",
        "       \n",
        "    return similarity\n",
        "  \n",
        "  # 키워드를 입력했을 때 단어 벡터 반환\n",
        "  def GetTermVector(self, term):\n",
        "    vec = self.term_mat[self.term2idx[term]:self.term2idx[term]+1,:]\n",
        "    print('{} = {}'.format(term, vec))\n",
        "    return vec\n",
        "  \n",
        "  # 문서를 입력했을 때 문서 벡터 반환\n",
        "  def GetDocVector(self, doc):\n",
        "    vec = self.doc_mat.T[self.doc2idx[doc]:self.doc2idx[doc]+1,:]\n",
        "    print('{} = {}'.format(doc, vec))\n",
        "    return vec\n",
        "  \n",
        "  def Compression(self, round_num=0) :\n",
        "    print(self.tdm)\n",
        "    print(self.term_doc_mat.round(round_num))\n",
        "   \n",
        "  def TermVectorMatrix(self, u, s, topic_num):\n",
        "    term_mat = np.matrix(u[:, :topic_num])# * np.diag(s[:topic_num])\n",
        "    return term_mat\n",
        "  \n",
        "  def DocVectorMatrix(self, s, vt, topic_num):\n",
        "    doc_mat = np.diag(s[:topic_num]) * np.matrix(vt[:topic_num,:])\n",
        "    return doc_mat\n",
        "  \n",
        "  def TermDocVectorMatrix(self, u, s, vt, topic_num):\n",
        "    term_doc_mat = np.matrix(u[:, :topic_num]) * np.diag(s[:topic_num])  * np.matrix(vt[:topic_num,:])\n",
        "    return term_doc_mat\n",
        "  \n",
        "  def TermSimilarityMatrix(self, termVectorMatrix):\n",
        "    return self.vectorSimilarity(termVectorMatrix.T)\n",
        "  \n",
        "  def GetTermSimilarity(self, term1, term2):\n",
        "    sim = self.term_sim[self.term2idx[term1], self.term2idx[term2]]\n",
        "    print(\"({},{}) term similarity = {}\".format(term1, term2, sim))\n",
        "    return sim \n",
        "  \n",
        "  def DocSimilarityMartrix(self,docVectorMatrix):    \n",
        "    return self.vectorSimilarity(docVectorMatrix) \n",
        "  \n",
        "  def GetDocSimilarity(self, doc1, doc2):\n",
        "    sim = self.doc_sim[self.doc2idx[doc1], self.doc2idx[doc2]]\n",
        "    print(\"('{}','{}') doc similarity = {}\".format(doc1, doc2, sim))\n",
        "    return sim "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqZnr8SX86qt",
        "colab_type": "code",
        "outputId": "99e5e38b-e04b-46be-e36e-51230b64aab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "doc_ls = ['바나나 사과 포도 포도 짜장면',\n",
        "         '사과 포도',\n",
        "         '포도 바나나',\n",
        "         '짜장면 짬뽕 탕수육',\n",
        "         '볶음밥 탕수육',\n",
        "         '짜장면 짬뽕',\n",
        "         '라면 스시',\n",
        "         '스시 짜장면',\n",
        "         '가츠동 스시 소바',\n",
        "         '된장찌개 김치찌개 김치',\n",
        "         '김치 된장 짜장면',\n",
        "         '비빔밥 김치'\n",
        "         ]\n",
        "\n",
        "lsa = LSA(doc_ls, 4)\n",
        "X = lsa.TDM(doc_ls)\n",
        "print('== 토픽 모델링 ==')\n",
        "lsa.TopicModeling(4)\n",
        "print('\\n== 단어 벡터 ==')\n",
        "lsa.GetTermVector('사과')\n",
        "lsa.GetTermVector('짜장면')\n",
        "print('\\n== 단어 유사도 ==')\n",
        "lsa.GetTermSimilarity('사과','바나나')\n",
        "lsa.GetTermSimilarity('사과','짜장면')\n",
        "lsa.GetTermSimilarity('포도','짜장면')\n",
        "lsa.GetTermSimilarity('사과','스시')\n",
        "print('\\n== 문서 벡터 ==')\n",
        "lsa.GetDocVector('사과 포도')\n",
        "lsa.GetDocVector('짜장면 짬뽕')\n",
        "print('\\n== 문서 유사도 ==')\n",
        "lsa.GetDocSimilarity('사과 포도', '포도 바나나')\n",
        "lsa.GetDocSimilarity('사과 포도', '라면 스시')\n",
        "print('\\n== 토픽 차원수로 압축 ==')\n",
        "lsa.Compression(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== 토픽 모델링 ==\n",
            "Topic 1 - [('포도', 0.697), ('짜장면', 0.486), ('사과', 0.348), ('바나나', 0.348)]\n",
            "Topic 2 - [('짜장면', 0.584), ('짬뽕', 0.356), ('김치', 0.337), ('스시', 0.256)]\n",
            "Topic 3 - [('김치', 0.611), ('된장찌개', 0.264), ('김치찌개', 0.264), ('비빔밥', 0.185)]\n",
            "Topic 4 - [('스시', 0.552), ('김치', 0.371), ('가츠동', 0.277), ('소바', 0.277)]\n",
            "\n",
            "== 단어 벡터 ==\n",
            "사과 = [[ 0.34843127 -0.19370961  0.01592593  0.03744775]]\n",
            "짜장면 = [[ 0.48563449  0.58415588 -0.07468389 -0.18737521]]\n",
            "\n",
            "== 단어 유사도 ==\n",
            "(사과,바나나) term similarity = 1.0\n",
            "(사과,짜장면) term similarity = 0.1519133521480699\n",
            "(포도,짜장면) term similarity = 0.15191335214807006\n",
            "(사과,스시) term similarity = -0.04097825202732752\n",
            "\n",
            "== 문서 벡터 ==\n",
            "사과 포도 = [[ 1.04529381 -0.58112882  0.04777778  0.11234324]]\n",
            "짜장면 짬뽕 = [[ 0.61011838  0.93971158 -0.17760018 -0.53682795]]\n",
            "\n",
            "== 문서 유사도 ==\n",
            "('사과 포도','포도 바나나') doc similarity = 0.9999999999999998\n",
            "('사과 포도','라면 스시') doc similarity = -0.03850688211350069\n",
            "\n",
            "== 토픽 차원수로 압축 ==\n",
            "[[1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [2. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "[[ 1.  0.  0. -0. -0.  0. -0.  0. -0. -0.  0. -0.]\n",
            " [ 1.  0.  0. -0. -0.  0. -0.  0. -0. -0.  0. -0.]\n",
            " [ 2.  1.  1. -0. -0.  0. -0.  0. -0. -0.  0. -0.]\n",
            " [ 1.  0.  0.  1.  0.  1.  0.  1.  0.  0.  1.  0.]\n",
            " [ 0. -0. -0.  1.  0.  1. -0.  0. -0. -0.  0. -0.]\n",
            " [ 0. -0. -0.  1.  0.  0. -0.  0. -0. -0.  0. -0.]\n",
            " [-0. -0. -0.  0.  0.  0. -0.  0. -0. -0.  0. -0.]\n",
            " [-0. -0. -0. -0. -0. -0.  0.  0.  0. -0. -0. -0.]\n",
            " [ 0. -0. -0.  0. -0.  0.  1.  1.  1. -0.  0. -0.]\n",
            " [-0. -0. -0. -0. -0. -0.  0.  0.  1. -0. -0. -0.]\n",
            " [-0. -0. -0. -0. -0. -0.  0.  0.  1. -0. -0. -0.]\n",
            " [-0. -0. -0. -0. -0. -0. -0. -0. -0.  0.  0.  0.]\n",
            " [-0. -0. -0. -0. -0. -0. -0. -0. -0.  0.  0.  0.]\n",
            " [ 0. -0. -0. -0. -0.  0. -0.  0. -0.  1.  1.  1.]\n",
            " [ 0. -0. -0.  0.  0.  0. -0.  0. -0.  0.  0.  0.]\n",
            " [-0. -0. -0. -0. -0. -0. -0. -0. -0.  0.  0.  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31Kn3iEWLDz9",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 sklearn LSA 구현 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LVVXaR4EJCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_ls = ['바나나 사과 포도 포도',\n",
        "         '사과 포도',\n",
        "         '포도 바나나',\n",
        "         '짜장면 짬뽕 탕수욕',\n",
        "         '볶음밥 탕수욕',\n",
        "         '짜장면 짬뽕',\n",
        "         '라면 스시',\n",
        "         '스시',\n",
        "         '가츠동 스시 소바',\n",
        "         '된장찌개 김치찌개 김치',\n",
        "         '김치 된장',\n",
        "         '비빔밥 김치'\n",
        "         ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOPy5VKzKr8_",
        "colab_type": "code",
        "outputId": "573e43c8-3150-4d72-cb80-974d70d3fffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "max_features= 1000, # 상위 1,000개의 단어를 보존 \n",
        "max_df = 0.5, \n",
        "smooth_idf=True)\n",
        "\n",
        "X = vectorizer.fit_transform(doc_ls)\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "svd_model = TruncatedSVD(n_components=4, algorithm='randomized', n_iter=100)\n",
        "svd_model.fit(X)\n",
        "\n",
        "np.shape(svd_model.components_)\n",
        "\n",
        "terms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\n",
        "\n",
        "def get_topics(components, feature_names, n=3):\n",
        "    for idx, topic in enumerate(components):\n",
        "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
        "get_topics(svd_model.components_,terms)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 1: [('포도', 0.78069), ('사과', 0.44189), ('바나나', 0.44189)]\n",
            "Topic 2: [('스시', 0.8864), ('라면', 0.33189), ('가츠동', 0.2282)]\n",
            "Topic 3: [('짬뽕', 0.6258), ('짜장면', 0.6258), ('탕수욕', 0.43614)]\n",
            "Topic 4: [('김치', 0.76421), ('된장', 0.37119), ('비빔밥', 0.37119)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATfiKPg2wHkQ",
        "colab_type": "text"
      },
      "source": [
        "# 2 LDA 실습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTdsTBi5qO6x",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 실습 템플릿"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3F7SDbJsrve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LDA :\n",
        "  def __init__(self, doc_ls, topic_num, alpha = 0.1, beta = 0.001):\n",
        "    self.alpha = alpha\n",
        "    self.beta = beta\n",
        "    self.k = topic_num\n",
        "  \n",
        "  def RandomlyAssignTopic(self, doc_ls):\n",
        "    pass\n",
        "  \n",
        "\n",
        "  def IterateAssignTopic(self) :\n",
        "    pass\n",
        "  \n",
        "  \n",
        "  # 토픽별 주요 키워드 출력\n",
        "  def TopicModeling(self) :\n",
        "    pass\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS7TS1QP9Q7e",
        "colab_type": "text"
      },
      "source": [
        "##2.2  실습 예제 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj6SMDULtUgd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "7bea1d54-a9b0-420d-cbc1-d1555fec6b65"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxl6Bat79QQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords  \n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "\n",
        "class LDA :\n",
        "  def __init__(self, docs, topic_num, alpha = 0.1, beta = 0.001):\n",
        "    self.alpha = alpha\n",
        "    self.beta = beta\n",
        "    self.k = topic_num\n",
        "    self.docs = docs\n",
        "    \n",
        " \n",
        "  def RandomlyAssignTopic(self, docs) :\n",
        "    dic = defaultdict(dict)\n",
        "    t2i = defaultdict(lambda : len(t2i))\n",
        "    i2t = defaultdict()\n",
        "    d = 0\n",
        "    w = 0\n",
        "    \n",
        "    wnl = WordNetLemmatizer()\n",
        "    stopword = stopwords.words('english')\n",
        "    stopword.append(',')\n",
        "    \n",
        "    # 임의의 토픽을 할당\n",
        "    for tokens in [word_tokenize(doc) for doc in docs] :\n",
        "      for token in [wnl.lemmatize(token.lower()) for token in tokens if token not in stopword] :\n",
        "        i2t[t2i[token]] = token\n",
        "        dic[(d, t2i[token], w)] = random.randint(0,self.k-1)\n",
        "        w += 1\n",
        "      d += 1\n",
        "  \n",
        "    #print(dic)\n",
        "    return dic, t2i, i2t\n",
        "  \n",
        "  \n",
        "  def CountDocTopic(self) :\n",
        "    docs = np.zeros((self.k, len(self.docs)))\n",
        "    terms = np.zeros((self.k, len(self.t2i.keys())))\n",
        "\n",
        "    \n",
        "    #문서별 토큰별 빈도수 계산\n",
        "    for (d, n, w) in self.term_topic.keys() :\n",
        "      docs[self.term_topic[(d, n, w)], d] += 1 + self.alpha\n",
        "      terms[self.term_topic[(d, n, w)], d] += 1 + self.beta\n",
        "    #print(doc_m)\n",
        "    \n",
        "    #비어있는 값는 값에 alpha, beta 설정\n",
        "    docs = np.where(docs==0.0, self.alpha, docs) \n",
        "    terms = np.where(terms==0.0, self.beta, terms)\n",
        "      \n",
        "    return docs, terms\n",
        "  \n",
        "  \n",
        "  def IterateAssignTopic(self, docs, terms) :\n",
        "    #한개의 단어씩 주제 배정\n",
        "    prev = {}\n",
        "    \n",
        "    while prev != self.term_topic:\n",
        "      for (d, n, w) in self.term_topic.keys() :\n",
        "        topic = [0, 0]\n",
        "\n",
        "        docs[self.term_topic[(d, n, w)], d] -= (1 + self.alpha)\n",
        "        terms[self.term_topic[(d, n, w)], n] -= (1 + self.beta)\n",
        "\n",
        "        #print(doc_m)\n",
        "        prev = self.term_topic\n",
        "        \n",
        "        for t in range(self.k) :\n",
        "          p_t_d = docs[t, d]/docs[:,d:d+1].sum()\n",
        "          p_w_t = terms[t, n]/terms[t:t+1,:].sum()\n",
        "          prob = p_t_d * p_w_t\n",
        "\n",
        "          if topic[1] < prob :\n",
        "            topic = [t, prob]\n",
        "\n",
        "        #확률이 가장 큰 토픽을 할당\n",
        "        self.term_topic[(d, n, w)] = topic[0]\n",
        "        docs[self.term_topic[(d, n, w)], d] += (1 + self.alpha)\n",
        "        terms[self.term_topic[(d, n, w)], t] += (1 + self.beta)\n",
        "        \n",
        "        #print(self.term_topic)\n",
        "        \n",
        "    return terms\n",
        "\n",
        "  \n",
        "  # 토픽별 주요 키워드 출력\n",
        "  def TopicModeling(self, count=5) :\n",
        "    self.term_topic, self.t2i, self.i2t = self.RandomlyAssignTopic(self.docs)\n",
        "    docs, terms = self.CountDocTopic()\n",
        "    terms = self.IterateAssignTopic(docs, terms)\n",
        "    \n",
        "    score = terms / terms.sum(axis=1, keepdims=True)\n",
        "    \n",
        "    for i in range(self.k) :\n",
        "      print(\"\\nTopic {}\".format(i+1))\n",
        "      sorted_index = np.flip(np.argsort(score[i]),0)[:count]\n",
        "      for j in sorted_index :\n",
        "        print(\"({}={})\".format(self.i2t[j], score[i,j].round(3)), end = ' ')\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sfF7Quwrv9m",
        "colab_type": "code",
        "outputId": "83960f5a-d139-40c6-da4d-33b48b5ee94b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "doc_ls = [\"Cute kitty\",\n",
        "\"Eat rice or cake\",\n",
        "\"Kitty and hamster\",\n",
        "\"Eat bread\",\n",
        "\"Rice, bread and cake\",\n",
        "\"Cute hamster eats bread and cake\"]\n",
        "\n",
        "lda = LDA(doc_ls, 2)\n",
        "lda.TopicModeling()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Topic 1\n",
            "(kitty=1.071) (cake=0.071) (cute=0.071) (eats=0.0) (hamster=0.0) \n",
            "Topic 2\n",
            "(hamster=0.999) (kitty=0.999) (eat=0.333) (rice=0.0) (eats=-0.333) "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi0kPw1isUwj",
        "colab_type": "text"
      },
      "source": [
        "##2.3 pyLDAvis 를 이용한 LDA 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3aI6QjjtFDT",
        "colab_type": "code",
        "outputId": "d4f898d7-651e-42c0-8abe-112184ed8439",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.16.4)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.3.0)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.24.2)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.13.2)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.10.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.6.9)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/3a/fc8323f913e8a9c6f33f7203547f8a2171223da5ed965f2541dafb10aa09/funcy-1.13-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (19.1.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (7.2.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (41.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.12.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=617ee3e2dc929f7769c17bcafd79cbac36e1cef1d242e880788a481c416c9ea6\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.13 pyLDAvis-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hza4_0k8skKB",
        "colab_type": "code",
        "outputId": "0f6b4fb6-f2b1-4a31-baa3-07ef5cc65728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data\n",
        "len(documents)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJMQKp0pssrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df = pd.DataFrame({'document':documents})\n",
        "# 특수 문자 제거\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "# 전체 단어에 대한 소문자 변환\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XAbHXJQsvGp",
        "colab_type": "code",
        "outputId": "f280f236-5a7e-4388-914f-76937069cb5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english') # NLTK로부터 불용어를 받아옵니다.\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhbqVu5esxdW",
        "colab_type": "code",
        "outputId": "b7b3cfce-1aac-4fb4-c9bf-cfb1d4e9b936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from gensim import corpora\n",
        "dictionary = corpora.Dictionary(tokenized_doc)\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
        "print(corpus[1]) # 수행된 결과에서 두번째 뉴스 출력. 첫번째 문서의 인덱스는 0"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(52, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 2), (67, 1), (68, 1), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 2), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 2), (86, 1), (87, 1), (88, 1), (89, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVMkTWb0tCEW",
        "colab_type": "code",
        "outputId": "85cad7af-29ad-4eac-c8bd-180f2f771f2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "import gensim\n",
        "NUM_TOPICS = 20 #20개의 토픽, k=20\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
        "topics = ldamodel.print_topics(num_words=4)\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, '0.011*\"health\" + 0.009*\"medical\" + 0.006*\"pain\" + 0.006*\"disease\"')\n",
            "(1, '0.016*\"game\" + 0.014*\"team\" + 0.012*\"year\" + 0.010*\"games\"')\n",
            "(2, '0.010*\"armenian\" + 0.009*\"israel\" + 0.009*\"jews\" + 0.009*\"armenians\"')\n",
            "(3, '0.010*\"windows\" + 0.008*\"available\" + 0.008*\"software\" + 0.007*\"also\"')\n",
            "(4, '0.040*\"file\" + 0.021*\"entry\" + 0.012*\"section\" + 0.011*\"program\"')\n",
            "(5, '0.005*\"phillies\" + 0.005*\"plane\" + 0.004*\"outbreak\" + 0.004*\"aged\"')\n",
            "(6, '0.011*\"would\" + 0.009*\"people\" + 0.007*\"think\" + 0.007*\"jesus\"')\n",
            "(7, '0.007*\"wiring\" + 0.005*\"reply\" + 0.004*\"word\" + 0.004*\"homosexual\"')\n",
            "(8, '0.032*\"drive\" + 0.024*\"card\" + 0.020*\"scsi\" + 0.013*\"disk\"')\n",
            "(9, '0.034*\"output\" + 0.011*\"printf\" + 0.011*\"char\" + 0.011*\"oname\"')\n",
            "(10, '0.008*\"system\" + 0.006*\"less\" + 0.005*\"picture\" + 0.005*\"much\"')\n",
            "(11, '0.007*\"encryption\" + 0.007*\"system\" + 0.007*\"chip\" + 0.007*\"government\"')\n",
            "(12, '0.040*\"space\" + 0.015*\"nasa\" + 0.008*\"launch\" + 0.007*\"earth\"')\n",
            "(13, '0.015*\"university\" + 0.010*\"list\" + 0.009*\"april\" + 0.009*\"mail\"')\n",
            "(14, '0.014*\"like\" + 0.014*\"would\" + 0.010*\"good\" + 0.010*\"know\"')\n",
            "(15, '0.012*\"period\" + 0.011*\"water\" + 0.011*\"power\" + 0.010*\"ground\"')\n",
            "(16, '0.005*\"hitter\" + 0.004*\"stewart\" + 0.003*\"rochester\" + 0.003*\"giants\"')\n",
            "(17, '0.014*\"sale\" + 0.010*\"offer\" + 0.010*\"shipping\" + 0.009*\"condition\"')\n",
            "(18, '0.013*\"people\" + 0.011*\"would\" + 0.007*\"said\" + 0.006*\"know\"')\n",
            "(19, '0.011*\"nrhj\" + 0.007*\"wwiz\" + 0.006*\"bxom\" + 0.006*\"gizw\"')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP20CapFtOD5",
        "colab_type": "code",
        "outputId": "c0571a7f-946f-472d-ae37-630b8ca3f5ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "import pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
        "pyLDAvis.display(vis)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-063ee2783176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    396\u001b[0m    \u001b[0mterm_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m    \u001b[0mtopic_info\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0m_topic_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m    \u001b[0mtoken_table\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0m_token_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m    \u001b[0mtopic_coordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_topic_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m    top_terms = pd.concat(Parallel(n_jobs=n_jobs)(delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls) \\\n\u001b[0;32m--> 255\u001b[0;31m                                                  for ls in _job_chunks(lambda_seq, n_jobs)))\n\u001b[0m\u001b[1;32m    256\u001b[0m    \u001b[0mtopic_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_top_term_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_terms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdefault_term_info\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_dfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJjzXbWrtOkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, topic_list in enumerate(ldamodel[corpus]):\n",
        "    if i==5:\n",
        "        break\n",
        "    print(i,'번째 문서의 topic 비율은',topic_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxkdBgjStRfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_topictable_per_doc(ldamodel, corpus, texts):\n",
        "    topic_table = pd.DataFrame()\n",
        "\n",
        "    # 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.\n",
        "    for i, topic_list in enumerate(ldamodel[corpus]):\n",
        "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n",
        "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n",
        "        # 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.\n",
        "        # EX) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%), \n",
        "        # Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)\n",
        "        # 48 > 25 > 21 > 5 순으로 정렬이 된 것.\n",
        "\n",
        "        # 모든 문서에 대해서 각각 아래를 수행\n",
        "        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.\n",
        "            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽\n",
        "                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n",
        "                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.\n",
        "            else:\n",
        "                break\n",
        "    return(topic_table)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSFPSOXztUH0",
        "colab_type": "code",
        "outputId": "01e766eb-5aec-48b9-cc62-03cffffd29df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "topictable = make_topictable_per_doc(ldamodel, corpus, tokenized_doc)\n",
        "topictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\n",
        "topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\n",
        "topictable[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>문서 번호</th>\n",
              "      <th>가장 비중이 높은 토픽</th>\n",
              "      <th>가장 높은 토픽의 비중</th>\n",
              "      <th>각 토픽의 비중</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.6845</td>\n",
              "      <td>[(8, 0.30094197), (17, 0.6845419)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.5632</td>\n",
              "      <td>[(2, 0.029335152), (8, 0.5632338), (10, 0.1083...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.6275</td>\n",
              "      <td>[(4, 0.018661937), (8, 0.62747276), (16, 0.045...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.2927</td>\n",
              "      <td>[(1, 0.044962943), (8, 0.14445594), (9, 0.0294...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.6536</td>\n",
              "      <td>[(2, 0.082709715), (5, 0.23224762), (10, 0.653...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.6517</td>\n",
              "      <td>[(0, 0.105608456), (3, 0.15081134), (8, 0.6517...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.7437</td>\n",
              "      <td>[(2, 0.013339129), (7, 0.012894245), (8, 0.014...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.4894</td>\n",
              "      <td>[(8, 0.48941994), (10, 0.12249057), (13, 0.024...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.3653</td>\n",
              "      <td>[(0, 0.2194796), (3, 0.36532196), (8, 0.220569...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.4796</td>\n",
              "      <td>[(0, 0.038544882), (4, 0.04621158), (6, 0.0463...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   문서 번호  ...                                           각 토픽의 비중\n",
              "0      0  ...                 [(8, 0.30094197), (17, 0.6845419)]\n",
              "1      1  ...  [(2, 0.029335152), (8, 0.5632338), (10, 0.1083...\n",
              "2      2  ...  [(4, 0.018661937), (8, 0.62747276), (16, 0.045...\n",
              "3      3  ...  [(1, 0.044962943), (8, 0.14445594), (9, 0.0294...\n",
              "4      4  ...  [(2, 0.082709715), (5, 0.23224762), (10, 0.653...\n",
              "5      5  ...  [(0, 0.105608456), (3, 0.15081134), (8, 0.6517...\n",
              "6      6  ...  [(2, 0.013339129), (7, 0.012894245), (8, 0.014...\n",
              "7      7  ...  [(8, 0.48941994), (10, 0.12249057), (13, 0.024...\n",
              "8      8  ...  [(0, 0.2194796), (3, 0.36532196), (8, 0.220569...\n",
              "9      9  ...  [(0, 0.038544882), (4, 0.04621158), (6, 0.0463...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}