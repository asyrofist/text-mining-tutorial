{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_핵심 키워드 추출_Keyword Extraction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "YZ8Uu6Rqbfyi"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fininsight/text-mining-tutorial/blob/master/3_%ED%95%B5%EC%8B%AC_%ED%82%A4%EC%9B%8C%EB%93%9C_%EC%B6%94%EC%B6%9C_Keyword_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1Q9mWDrgMUn",
        "colab_type": "text"
      },
      "source": [
        "# 핵심 키워드 추출 (Keyword Extraction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxsBfWirA9ao",
        "colab_type": "text"
      },
      "source": [
        "# 1 TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of0dQ_ALpLw6",
        "colab_type": "text"
      },
      "source": [
        "### 1) 샘플 텍스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAYZY89FBsZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d1 = \"The cat sat on my face. I hate a cat.\"\n",
        "d2 = \"The dog sat on my bed. I love a dog.\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqZ9ONgLpOzv",
        "colab_type": "text"
      },
      "source": [
        "### 2) sklearn 활용 TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em3l3IS5kRP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "\n",
        "document_ls = [d1, d2]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf = vectorizer.fit_transform(document_ls)\n",
        "\n",
        "word2id = defaultdict(lambda : 0)\n",
        "for idx, feature in enumerate(vectorizer.get_feature_names()):\n",
        "    word2id[feature] = idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESM6EqgjpUbG",
        "colab_type": "text"
      },
      "source": [
        "### 3) dataframe으로 변환하여 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy9QLG-NlF5N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "295a643c-25f4-438a-b634-22a4e9751163"
      },
      "source": [
        "import pandas as pd\n",
        "count_vect_df = pd.DataFrame(tfidf.todense(), columns=vectorizer.get_feature_names())\n",
        "count_vect_df"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bed</th>\n",
              "      <th>cat</th>\n",
              "      <th>dog</th>\n",
              "      <th>face</th>\n",
              "      <th>hate</th>\n",
              "      <th>love</th>\n",
              "      <th>my</th>\n",
              "      <th>on</th>\n",
              "      <th>sat</th>\n",
              "      <th>the</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.706006</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.353003</td>\n",
              "      <td>0.353003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.251164</td>\n",
              "      <td>0.251164</td>\n",
              "      <td>0.251164</td>\n",
              "      <td>0.251164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.353003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.706006</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.353003</td>\n",
              "      <td>0.251164</td>\n",
              "      <td>0.251164</td>\n",
              "      <td>0.251164</td>\n",
              "      <td>0.251164</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        bed       cat       dog      face      hate      love        my  \\\n",
              "0  0.000000  0.706006  0.000000  0.353003  0.353003  0.000000  0.251164   \n",
              "1  0.353003  0.000000  0.706006  0.000000  0.000000  0.353003  0.251164   \n",
              "\n",
              "         on       sat       the  \n",
              "0  0.251164  0.251164  0.251164  \n",
              "1  0.251164  0.251164  0.251164  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9d0sJ2Vqpw-",
        "colab_type": "text"
      },
      "source": [
        "### 4) TF-IDF score가 높은 순으로 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBP6VjiylrjP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e8d66b6a-d8e9-4e1d-ce7d-3540381ed450"
      },
      "source": [
        "feature_array = np.array(vectorizer.get_feature_names())\n",
        "tfidf_sorting = np.argsort(tfidf[0].toarray()).flatten()[::-1]\n",
        "\n",
        "n = 3\n",
        "top_n = feature_array[tfidf_sorting][:n]\n",
        "\n",
        "top_n"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['cat', 'hate', 'face'], dtype='<U4')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZpm1wtTqxL-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfekkYshBAVS",
        "colab_type": "text"
      },
      "source": [
        "# 2 Textrank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn58myMFF-nN",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://i.stack.imgur.com/ohF5r.png\" />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X5YARYrBohs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ8Uu6Rqbfyi",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 TextRank 직접 구현하기\n",
        "(Based on: https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk06RCeMbfyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Source of text:\n",
        "#https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents\n",
        "\n",
        "Text = \"The FAANG stocks won’t see much more growth in the near future, according to Bill Studebaker, founder and Chief Investment Officer of Robo Global. \\\n",
        "Studebaker argues we are seeing a 'reallocation' that will continue from large-cap tech stocks into market-weight stocks. \\\n",
        "The FAANG stocks have had a rough few weeks, and have been hit hard since March 12. \\\n",
        "One FAANG to look out for, in the midst of all this, is Amazon, according to Studebaker. \\\n",
        "The stock market is seeing a 'reallocation' out of FAANG stocks, which are not where the smart money is, founder and Chief Investment Officer of Robo Global Bill Studebaker told Business Insider. \\\n",
        "The FAANG stocks (Facebook, Apple, Amazon, Netflix, Google) are all down considerably since March 12, a trend that accelerated when news of a massive Facebook data scandal broke, sending the tech-heavy Nasdaq into a downward frenzy. \\\n",
        "Investors are wondering what’s next. \\\n",
        "And what’s next isn’t good news for FAANG stock optimists, Studebaker thinks. 'This is a dead trade' for the next several months, he said. 'I wouldn’t expect there to be a lot of performance attribution coming from the FAANG stocks,' he added. That is, if the stock market is to see gains in the next several months, they will largely not come from the big tech companies. \\\n",
        "The market is seeing a 'reallocation out of large-cap technology, into other parts of the market,' he said. And this trend could continue for the foreseeable future. 'When you get these reallocation trades, a de-risking, this can go on for months and months.' The FAANG’s are pricey stocks, he said, pointing out that investors will 'factor in the law of big numbers,' he said. 'Just because they’re big cap doesn’t mean they’re safe,' he added. \\\n",
        "Still, he doesn’t necessarily think that investors are going to shift drastically into value stocks. 'With an increasingly favorable macro backdrop, you have strong growth demand.' \\\n",
        "Studebaker, who runs an artificial intelligence and robotics exchange-traded fund with $4 billion in assets under management, thinks that AI and robotics are better areas of growth. His ETF is up 27% in the past year, while the FAANG stocks are also largely up over that same span, even if they are down since March 12. \\\n",
        "While many point to artificial intelligence as an area that will be a boost to Google and Amazon, Studebaker doesn’t see that as a sign of significant growth for the FAANGs. He pointed out that 'eighty to ninety percent of their businesses are still search,' and that 'AI doesn’t really move the needle on the business.' He also said 'the revenue mix [attributable to AI] in those businesses are insignificant.' \\\n",
        "And while he’s not bullish on FAANG’s, he does say that the one FAANG to still watch out for is Amazon, simply because ecommerce still represents a small portion of the global retail market, giving the company room to grow.\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "683iA8igbfym",
        "colab_type": "text"
      },
      "source": [
        "### 1) 토큰화 (Tokenization)\n",
        "\n",
        "분석 텍스트 정제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SxuV1Zkbfyn",
        "colab_type": "code",
        "outputId": "63a35ca8-7dbe-46eb-ba99-7a361e72f7c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = word_tokenize(Text)\n",
        "\n",
        "print (\"Tokenized Text: \\n\")\n",
        "print (text)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Tokenized Text: \n",
            "\n",
            "['The', 'FAANG', 'stocks', 'won', '’', 't', 'see', 'much', 'more', 'growth', 'in', 'the', 'near', 'future', ',', 'according', 'to', 'Bill', 'Studebaker', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global', '.', 'Studebaker', 'argues', 'we', 'are', 'seeing', 'a', \"'reallocation\", \"'\", 'that', 'will', 'continue', 'from', 'large-cap', 'tech', 'stocks', 'into', 'market-weight', 'stocks', '.', 'The', 'FAANG', 'stocks', 'have', 'had', 'a', 'rough', 'few', 'weeks', ',', 'and', 'have', 'been', 'hit', 'hard', 'since', 'March', '12', '.', 'One', 'FAANG', 'to', 'look', 'out', 'for', ',', 'in', 'the', 'midst', 'of', 'all', 'this', ',', 'is', 'Amazon', ',', 'according', 'to', 'Studebaker', '.', 'The', 'stock', 'market', 'is', 'seeing', 'a', \"'reallocation\", \"'\", 'out', 'of', 'FAANG', 'stocks', ',', 'which', 'are', 'not', 'where', 'the', 'smart', 'money', 'is', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global', 'Bill', 'Studebaker', 'told', 'Business', 'Insider', '.', 'The', 'FAANG', 'stocks', '(', 'Facebook', ',', 'Apple', ',', 'Amazon', ',', 'Netflix', ',', 'Google', ')', 'are', 'all', 'down', 'considerably', 'since', 'March', '12', ',', 'a', 'trend', 'that', 'accelerated', 'when', 'news', 'of', 'a', 'massive', 'Facebook', 'data', 'scandal', 'broke', ',', 'sending', 'the', 'tech-heavy', 'Nasdaq', 'into', 'a', 'downward', 'frenzy', '.', 'Investors', 'are', 'wondering', 'what', '’', 's', 'next', '.', 'And', 'what', '’', 's', 'next', 'isn', '’', 't', 'good', 'news', 'for', 'FAANG', 'stock', 'optimists', ',', 'Studebaker', 'thinks', '.', \"'This\", 'is', 'a', 'dead', 'trade', \"'\", 'for', 'the', 'next', 'several', 'months', ',', 'he', 'said', '.', \"'I\", 'wouldn', '’', 't', 'expect', 'there', 'to', 'be', 'a', 'lot', 'of', 'performance', 'attribution', 'coming', 'from', 'the', 'FAANG', 'stocks', ',', \"'\", 'he', 'added', '.', 'That', 'is', ',', 'if', 'the', 'stock', 'market', 'is', 'to', 'see', 'gains', 'in', 'the', 'next', 'several', 'months', ',', 'they', 'will', 'largely', 'not', 'come', 'from', 'the', 'big', 'tech', 'companies', '.', 'The', 'market', 'is', 'seeing', 'a', \"'reallocation\", 'out', 'of', 'large-cap', 'technology', ',', 'into', 'other', 'parts', 'of', 'the', 'market', ',', \"'\", 'he', 'said', '.', 'And', 'this', 'trend', 'could', 'continue', 'for', 'the', 'foreseeable', 'future', '.', \"'When\", 'you', 'get', 'these', 'reallocation', 'trades', ',', 'a', 'de-risking', ',', 'this', 'can', 'go', 'on', 'for', 'months', 'and', 'months', '.', \"'\", 'The', 'FAANG', '’', 's', 'are', 'pricey', 'stocks', ',', 'he', 'said', ',', 'pointing', 'out', 'that', 'investors', 'will', \"'factor\", 'in', 'the', 'law', 'of', 'big', 'numbers', ',', \"'\", 'he', 'said', '.', \"'Just\", 'because', 'they', '’', 're', 'big', 'cap', 'doesn', '’', 't', 'mean', 'they', '’', 're', 'safe', ',', \"'\", 'he', 'added', '.', 'Still', ',', 'he', 'doesn', '’', 't', 'necessarily', 'think', 'that', 'investors', 'are', 'going', 'to', 'shift', 'drastically', 'into', 'value', 'stocks', '.', \"'With\", 'an', 'increasingly', 'favorable', 'macro', 'backdrop', ',', 'you', 'have', 'strong', 'growth', 'demand', '.', \"'\", 'Studebaker', ',', 'who', 'runs', 'an', 'artificial', 'intelligence', 'and', 'robotics', 'exchange-traded', 'fund', 'with', '$', '4', 'billion', 'in', 'assets', 'under', 'management', ',', 'thinks', 'that', 'AI', 'and', 'robotics', 'are', 'better', 'areas', 'of', 'growth', '.', 'His', 'ETF', 'is', 'up', '27', '%', 'in', 'the', 'past', 'year', ',', 'while', 'the', 'FAANG', 'stocks', 'are', 'also', 'largely', 'up', 'over', 'that', 'same', 'span', ',', 'even', 'if', 'they', 'are', 'down', 'since', 'March', '12', '.', 'While', 'many', 'point', 'to', 'artificial', 'intelligence', 'as', 'an', 'area', 'that', 'will', 'be', 'a', 'boost', 'to', 'Google', 'and', 'Amazon', ',', 'Studebaker', 'doesn', '’', 't', 'see', 'that', 'as', 'a', 'sign', 'of', 'significant', 'growth', 'for', 'the', 'FAANGs', '.', 'He', 'pointed', 'out', 'that', \"'eighty\", 'to', 'ninety', 'percent', 'of', 'their', 'businesses', 'are', 'still', 'search', ',', \"'\", 'and', 'that', \"'AI\", 'doesn', '’', 't', 'really', 'move', 'the', 'needle', 'on', 'the', 'business', '.', \"'\", 'He', 'also', 'said', \"'the\", 'revenue', 'mix', '[', 'attributable', 'to', 'AI', ']', 'in', 'those', 'businesses', 'are', 'insignificant', '.', \"'\", 'And', 'while', 'he', '’', 's', 'not', 'bullish', 'on', 'FAANG', '’', 's', ',', 'he', 'does', 'say', 'that', 'the', 'one', 'FAANG', 'to', 'still', 'watch', 'out', 'for', 'is', 'Amazon', ',', 'simply', 'because', 'ecommerce', 'still', 'represents', 'a', 'small', 'portion', 'of', 'the', 'global', 'retail', 'market', ',', 'giving', 'the', 'company', 'room', 'to', 'grow', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-Bj7zTHbfyr",
        "colab_type": "text"
      },
      "source": [
        "### 2) 품사부착 (POS Tagging)\n",
        "\n",
        "토큰화된 텍스트에 품사 부착\n",
        "\n",
        "http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV8RUcY_bfyr",
        "colab_type": "code",
        "outputId": "9370dfaa-f827-47d8-c13e-639d8502e917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "POS_tag = nltk.pos_tag(text)\n",
        "\n",
        "print(POS_tag)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[('The', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), ('won', 'VBD'), ('’', 'JJ'), ('t', 'NN'), ('see', 'VBP'), ('much', 'RB'), ('more', 'JJR'), ('growth', 'NN'), ('in', 'IN'), ('the', 'DT'), ('near', 'JJ'), ('future', 'NN'), (',', ','), ('according', 'VBG'), ('to', 'TO'), ('Bill', 'NNP'), ('Studebaker', 'NNP'), (',', ','), ('founder', 'NN'), ('and', 'CC'), ('Chief', 'NNP'), ('Investment', 'NNP'), ('Officer', 'NNP'), ('of', 'IN'), ('Robo', 'NNP'), ('Global', 'NNP'), ('.', '.'), ('Studebaker', 'NNP'), ('argues', 'VBZ'), ('we', 'PRP'), ('are', 'VBP'), ('seeing', 'VBG'), ('a', 'DT'), (\"'reallocation\", 'NN'), (\"'\", 'POS'), ('that', 'WDT'), ('will', 'MD'), ('continue', 'VB'), ('from', 'IN'), ('large-cap', 'JJ'), ('tech', 'NN'), ('stocks', 'NNS'), ('into', 'IN'), ('market-weight', 'JJ'), ('stocks', 'NNS'), ('.', '.'), ('The', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), ('have', 'VBP'), ('had', 'VBD'), ('a', 'DT'), ('rough', 'JJ'), ('few', 'JJ'), ('weeks', 'NNS'), (',', ','), ('and', 'CC'), ('have', 'VBP'), ('been', 'VBN'), ('hit', 'VBN'), ('hard', 'JJ'), ('since', 'IN'), ('March', 'NNP'), ('12', 'CD'), ('.', '.'), ('One', 'CD'), ('FAANG', 'NNP'), ('to', 'TO'), ('look', 'VB'), ('out', 'RP'), ('for', 'IN'), (',', ','), ('in', 'IN'), ('the', 'DT'), ('midst', 'NN'), ('of', 'IN'), ('all', 'PDT'), ('this', 'DT'), (',', ','), ('is', 'VBZ'), ('Amazon', 'NNP'), (',', ','), ('according', 'VBG'), ('to', 'TO'), ('Studebaker', 'NNP'), ('.', '.'), ('The', 'DT'), ('stock', 'NN'), ('market', 'NN'), ('is', 'VBZ'), ('seeing', 'VBG'), ('a', 'DT'), (\"'reallocation\", 'NN'), (\"'\", 'POS'), ('out', 'IN'), ('of', 'IN'), ('FAANG', 'NNP'), ('stocks', 'NNS'), (',', ','), ('which', 'WDT'), ('are', 'VBP'), ('not', 'RB'), ('where', 'WRB'), ('the', 'DT'), ('smart', 'JJ'), ('money', 'NN'), ('is', 'VBZ'), (',', ','), ('founder', 'NN'), ('and', 'CC'), ('Chief', 'NNP'), ('Investment', 'NNP'), ('Officer', 'NNP'), ('of', 'IN'), ('Robo', 'NNP'), ('Global', 'NNP'), ('Bill', 'NNP'), ('Studebaker', 'NNP'), ('told', 'VBD'), ('Business', 'NNP'), ('Insider', 'NNP'), ('.', '.'), ('The', 'DT'), ('FAANG', 'JJ'), ('stocks', 'NNS'), ('(', '('), ('Facebook', 'NNP'), (',', ','), ('Apple', 'NNP'), (',', ','), ('Amazon', 'NNP'), (',', ','), ('Netflix', 'NNP'), (',', ','), ('Google', 'NNP'), (')', ')'), ('are', 'VBP'), ('all', 'DT'), ('down', 'RB'), ('considerably', 'RB'), ('since', 'IN'), ('March', 'NNP'), ('12', 'CD'), (',', ','), ('a', 'DT'), ('trend', 'NN'), ('that', 'WDT'), ('accelerated', 'VBD'), ('when', 'WRB'), ('news', 'NN'), ('of', 'IN'), ('a', 'DT'), ('massive', 'JJ'), ('Facebook', 'NNP'), ('data', 'NN'), ('scandal', 'NN'), ('broke', 'VBD'), (',', ','), ('sending', 'VBG'), ('the', 'DT'), ('tech-heavy', 'JJ'), ('Nasdaq', 'NNP'), ('into', 'IN'), ('a', 'DT'), ('downward', 'JJ'), ('frenzy', 'NN'), ('.', '.'), ('Investors', 'NNS'), ('are', 'VBP'), ('wondering', 'VBG'), ('what', 'WP'), ('’', 'NNP'), ('s', 'VBD'), ('next', 'JJ'), ('.', '.'), ('And', 'CC'), ('what', 'WP'), ('’', 'NNP'), ('s', 'VBD'), ('next', 'JJ'), ('isn', 'NN'), ('’', 'NNP'), ('t', 'NN'), ('good', 'JJ'), ('news', 'NN'), ('for', 'IN'), ('FAANG', 'NNP'), ('stock', 'NN'), ('optimists', 'NNS'), (',', ','), ('Studebaker', 'NNP'), ('thinks', 'VBZ'), ('.', '.'), (\"'This\", \"''\"), ('is', 'VBZ'), ('a', 'DT'), ('dead', 'JJ'), ('trade', 'NN'), (\"'\", \"''\"), ('for', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('several', 'JJ'), ('months', 'NNS'), (',', ','), ('he', 'PRP'), ('said', 'VBD'), ('.', '.'), (\"'I\", 'CC'), ('wouldn', 'JJ'), ('’', 'NNP'), ('t', 'NN'), ('expect', 'VBP'), ('there', 'EX'), ('to', 'TO'), ('be', 'VB'), ('a', 'DT'), ('lot', 'NN'), ('of', 'IN'), ('performance', 'NN'), ('attribution', 'NN'), ('coming', 'VBG'), ('from', 'IN'), ('the', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), (',', ','), (\"'\", \"''\"), ('he', 'PRP'), ('added', 'VBD'), ('.', '.'), ('That', 'DT'), ('is', 'VBZ'), (',', ','), ('if', 'IN'), ('the', 'DT'), ('stock', 'NN'), ('market', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('see', 'VB'), ('gains', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('several', 'JJ'), ('months', 'NNS'), (',', ','), ('they', 'PRP'), ('will', 'MD'), ('largely', 'RB'), ('not', 'RB'), ('come', 'VBN'), ('from', 'IN'), ('the', 'DT'), ('big', 'JJ'), ('tech', 'NN'), ('companies', 'NNS'), ('.', '.'), ('The', 'DT'), ('market', 'NN'), ('is', 'VBZ'), ('seeing', 'VBG'), ('a', 'DT'), (\"'reallocation\", 'NN'), ('out', 'IN'), ('of', 'IN'), ('large-cap', 'JJ'), ('technology', 'NN'), (',', ','), ('into', 'IN'), ('other', 'JJ'), ('parts', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('market', 'NN'), (',', ','), (\"'\", \"''\"), ('he', 'PRP'), ('said', 'VBD'), ('.', '.'), ('And', 'CC'), ('this', 'DT'), ('trend', 'NN'), ('could', 'MD'), ('continue', 'VB'), ('for', 'IN'), ('the', 'DT'), ('foreseeable', 'JJ'), ('future', 'NN'), ('.', '.'), (\"'When\", 'CC'), ('you', 'PRP'), ('get', 'VBP'), ('these', 'DT'), ('reallocation', 'NN'), ('trades', 'NNS'), (',', ','), ('a', 'DT'), ('de-risking', 'NN'), (',', ','), ('this', 'DT'), ('can', 'MD'), ('go', 'VB'), ('on', 'IN'), ('for', 'IN'), ('months', 'NNS'), ('and', 'CC'), ('months', 'NNS'), ('.', '.'), (\"'\", \"''\"), ('The', 'DT'), ('FAANG', 'NNP'), ('’', 'NNP'), ('s', 'NN'), ('are', 'VBP'), ('pricey', 'JJ'), ('stocks', 'NNS'), (',', ','), ('he', 'PRP'), ('said', 'VBD'), (',', ','), ('pointing', 'VBG'), ('out', 'RP'), ('that', 'IN'), ('investors', 'NNS'), ('will', 'MD'), (\"'factor\", 'VB'), ('in', 'IN'), ('the', 'DT'), ('law', 'NN'), ('of', 'IN'), ('big', 'JJ'), ('numbers', 'NNS'), (',', ','), (\"'\", \"''\"), ('he', 'PRP'), ('said', 'VBD'), ('.', '.'), (\"'Just\", 'CC'), ('because', 'IN'), ('they', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('big', 'JJ'), ('cap', 'NN'), ('doesn', 'NN'), ('’', 'NNP'), ('t', 'NN'), ('mean', 'NN'), ('they', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('safe', 'JJ'), (',', ','), (\"'\", \"''\"), ('he', 'PRP'), ('added', 'VBD'), ('.', '.'), ('Still', 'RB'), (',', ','), ('he', 'PRP'), ('doesn', 'VBZ'), ('’', 'JJ'), ('t', 'NNS'), ('necessarily', 'RB'), ('think', 'VBP'), ('that', 'IN'), ('investors', 'NNS'), ('are', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('shift', 'VB'), ('drastically', 'RB'), ('into', 'IN'), ('value', 'NN'), ('stocks', 'NNS'), ('.', '.'), (\"'With\", \"''\"), ('an', 'DT'), ('increasingly', 'RB'), ('favorable', 'JJ'), ('macro', 'NN'), ('backdrop', 'NN'), (',', ','), ('you', 'PRP'), ('have', 'VBP'), ('strong', 'JJ'), ('growth', 'NN'), ('demand', 'NN'), ('.', '.'), (\"'\", \"''\"), ('Studebaker', 'NNP'), (',', ','), ('who', 'WP'), ('runs', 'VBZ'), ('an', 'DT'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('and', 'CC'), ('robotics', 'NNS'), ('exchange-traded', 'JJ'), ('fund', 'NN'), ('with', 'IN'), ('$', '$'), ('4', 'CD'), ('billion', 'CD'), ('in', 'IN'), ('assets', 'NNS'), ('under', 'IN'), ('management', 'NN'), (',', ','), ('thinks', 'VBZ'), ('that', 'IN'), ('AI', 'NNP'), ('and', 'CC'), ('robotics', 'NNS'), ('are', 'VBP'), ('better', 'JJR'), ('areas', 'NNS'), ('of', 'IN'), ('growth', 'NN'), ('.', '.'), ('His', 'PRP$'), ('ETF', 'NN'), ('is', 'VBZ'), ('up', 'RP'), ('27', 'CD'), ('%', 'NN'), ('in', 'IN'), ('the', 'DT'), ('past', 'JJ'), ('year', 'NN'), (',', ','), ('while', 'IN'), ('the', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), ('are', 'VBP'), ('also', 'RB'), ('largely', 'RB'), ('up', 'IN'), ('over', 'IN'), ('that', 'DT'), ('same', 'JJ'), ('span', 'NN'), (',', ','), ('even', 'RB'), ('if', 'IN'), ('they', 'PRP'), ('are', 'VBP'), ('down', 'JJ'), ('since', 'IN'), ('March', 'NNP'), ('12', 'CD'), ('.', '.'), ('While', 'IN'), ('many', 'JJ'), ('point', 'NN'), ('to', 'TO'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('as', 'IN'), ('an', 'DT'), ('area', 'NN'), ('that', 'WDT'), ('will', 'MD'), ('be', 'VB'), ('a', 'DT'), ('boost', 'NN'), ('to', 'TO'), ('Google', 'NNP'), ('and', 'CC'), ('Amazon', 'NNP'), (',', ','), ('Studebaker', 'NNP'), ('doesn', 'VBZ'), ('’', 'JJ'), ('t', 'NNS'), ('see', 'VBP'), ('that', 'IN'), ('as', 'IN'), ('a', 'DT'), ('sign', 'NN'), ('of', 'IN'), ('significant', 'JJ'), ('growth', 'NN'), ('for', 'IN'), ('the', 'DT'), ('FAANGs', 'NNP'), ('.', '.'), ('He', 'PRP'), ('pointed', 'VBD'), ('out', 'RP'), ('that', 'IN'), (\"'eighty\", 'VBZ'), ('to', 'TO'), ('ninety', 'VB'), ('percent', 'NN'), ('of', 'IN'), ('their', 'PRP$'), ('businesses', 'NNS'), ('are', 'VBP'), ('still', 'RB'), ('search', 'RB'), (',', ','), (\"'\", \"''\"), ('and', 'CC'), ('that', 'IN'), (\"'AI\", 'NNP'), ('doesn', 'VBD'), ('’', 'NNP'), ('t', 'NN'), ('really', 'RB'), ('move', 'VB'), ('the', 'DT'), ('needle', 'NN'), ('on', 'IN'), ('the', 'DT'), ('business', 'NN'), ('.', '.'), (\"'\", \"''\"), ('He', 'PRP'), ('also', 'RB'), ('said', 'VBD'), (\"'the\", 'JJ'), ('revenue', 'NN'), ('mix', 'NN'), ('[', 'NNP'), ('attributable', 'NN'), ('to', 'TO'), ('AI', 'NNP'), (']', 'NNP'), ('in', 'IN'), ('those', 'DT'), ('businesses', 'NNS'), ('are', 'VBP'), ('insignificant', 'JJ'), ('.', '.'), (\"'\", \"''\"), ('And', 'CC'), ('while', 'IN'), ('he', 'PRP'), ('’', 'VBZ'), ('s', 'PRP'), ('not', 'RB'), ('bullish', 'VB'), ('on', 'IN'), ('FAANG', 'NNP'), ('’', 'NNP'), ('s', 'NN'), (',', ','), ('he', 'PRP'), ('does', 'VBZ'), ('say', 'VB'), ('that', 'IN'), ('the', 'DT'), ('one', 'CD'), ('FAANG', 'NNP'), ('to', 'TO'), ('still', 'RB'), ('watch', 'VB'), ('out', 'RP'), ('for', 'IN'), ('is', 'VBZ'), ('Amazon', 'NNP'), (',', ','), ('simply', 'RB'), ('because', 'IN'), ('ecommerce', 'NN'), ('still', 'RB'), ('represents', 'VBZ'), ('a', 'DT'), ('small', 'JJ'), ('portion', 'NN'), ('of', 'IN'), ('the', 'DT'), ('global', 'JJ'), ('retail', 'JJ'), ('market', 'NN'), (',', ','), ('giving', 'VBG'), ('the', 'DT'), ('company', 'NN'), ('room', 'NN'), ('to', 'TO'), ('grow', 'VB'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go4mXtQvbfyu",
        "colab_type": "text"
      },
      "source": [
        "### 3) 표제어 추출 (Lemmatization)\n",
        "    \n",
        "https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yynPU39Obfyu",
        "colab_type": "code",
        "outputId": "7c023b18-96cf-4390-8362-04d79983e97e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_text = []\n",
        "for word in POS_tag:\n",
        "  lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0])))\n",
        "        \n",
        "print(lemmatized_text)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "['The', 'FAANG', 'stock', 'won', '’', 't', 'see', 'much', 'more', 'growth', 'in', 'the', 'near', 'future', ',', 'according', 'to', 'Bill', 'Studebaker', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global', '.', 'Studebaker', 'argues', 'we', 'are', 'seeing', 'a', \"'reallocation\", \"'\", 'that', 'will', 'continue', 'from', 'large-cap', 'tech', 'stock', 'into', 'market-weight', 'stock', '.', 'The', 'FAANG', 'stock', 'have', 'had', 'a', 'rough', 'few', 'week', ',', 'and', 'have', 'been', 'hit', 'hard', 'since', 'March', '12', '.', 'One', 'FAANG', 'to', 'look', 'out', 'for', ',', 'in', 'the', 'midst', 'of', 'all', 'this', ',', 'is', 'Amazon', ',', 'according', 'to', 'Studebaker', '.', 'The', 'stock', 'market', 'is', 'seeing', 'a', \"'reallocation\", \"'\", 'out', 'of', 'FAANG', 'stock', ',', 'which', 'are', 'not', 'where', 'the', 'smart', 'money', 'is', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global', 'Bill', 'Studebaker', 'told', 'Business', 'Insider', '.', 'The', 'FAANG', 'stock', '(', 'Facebook', ',', 'Apple', ',', 'Amazon', ',', 'Netflix', ',', 'Google', ')', 'are', 'all', 'down', 'considerably', 'since', 'March', '12', ',', 'a', 'trend', 'that', 'accelerated', 'when', 'news', 'of', 'a', 'massive', 'Facebook', 'data', 'scandal', 'broke', ',', 'sending', 'the', 'tech-heavy', 'Nasdaq', 'into', 'a', 'downward', 'frenzy', '.', 'Investors', 'are', 'wondering', 'what', '’', 's', 'next', '.', 'And', 'what', '’', 's', 'next', 'isn', '’', 't', 'good', 'news', 'for', 'FAANG', 'stock', 'optimist', ',', 'Studebaker', 'think', '.', \"'This\", 'is', 'a', 'dead', 'trade', \"'\", 'for', 'the', 'next', 'several', 'month', ',', 'he', 'said', '.', \"'I\", 'wouldn', '’', 't', 'expect', 'there', 'to', 'be', 'a', 'lot', 'of', 'performance', 'attribution', 'coming', 'from', 'the', 'FAANG', 'stock', ',', \"'\", 'he', 'added', '.', 'That', 'is', ',', 'if', 'the', 'stock', 'market', 'is', 'to', 'see', 'gain', 'in', 'the', 'next', 'several', 'month', ',', 'they', 'will', 'largely', 'not', 'come', 'from', 'the', 'big', 'tech', 'company', '.', 'The', 'market', 'is', 'seeing', 'a', \"'reallocation\", 'out', 'of', 'large-cap', 'technology', ',', 'into', 'other', 'part', 'of', 'the', 'market', ',', \"'\", 'he', 'said', '.', 'And', 'this', 'trend', 'could', 'continue', 'for', 'the', 'foreseeable', 'future', '.', \"'When\", 'you', 'get', 'these', 'reallocation', 'trade', ',', 'a', 'de-risking', ',', 'this', 'can', 'go', 'on', 'for', 'month', 'and', 'month', '.', \"'\", 'The', 'FAANG', '’', 's', 'are', 'pricey', 'stock', ',', 'he', 'said', ',', 'pointing', 'out', 'that', 'investor', 'will', \"'factor\", 'in', 'the', 'law', 'of', 'big', 'number', ',', \"'\", 'he', 'said', '.', \"'Just\", 'because', 'they', '’', 're', 'big', 'cap', 'doesn', '’', 't', 'mean', 'they', '’', 're', 'safe', ',', \"'\", 'he', 'added', '.', 'Still', ',', 'he', 'doesn', '’', 't', 'necessarily', 'think', 'that', 'investor', 'are', 'going', 'to', 'shift', 'drastically', 'into', 'value', 'stock', '.', \"'With\", 'an', 'increasingly', 'favorable', 'macro', 'backdrop', ',', 'you', 'have', 'strong', 'growth', 'demand', '.', \"'\", 'Studebaker', ',', 'who', 'run', 'an', 'artificial', 'intelligence', 'and', 'robotics', 'exchange-traded', 'fund', 'with', '$', '4', 'billion', 'in', 'asset', 'under', 'management', ',', 'think', 'that', 'AI', 'and', 'robotics', 'are', 'better', 'area', 'of', 'growth', '.', 'His', 'ETF', 'is', 'up', '27', '%', 'in', 'the', 'past', 'year', ',', 'while', 'the', 'FAANG', 'stock', 'are', 'also', 'largely', 'up', 'over', 'that', 'same', 'span', ',', 'even', 'if', 'they', 'are', 'down', 'since', 'March', '12', '.', 'While', 'many', 'point', 'to', 'artificial', 'intelligence', 'a', 'an', 'area', 'that', 'will', 'be', 'a', 'boost', 'to', 'Google', 'and', 'Amazon', ',', 'Studebaker', 'doesn', '’', 't', 'see', 'that', 'a', 'a', 'sign', 'of', 'significant', 'growth', 'for', 'the', 'FAANGs', '.', 'He', 'pointed', 'out', 'that', \"'eighty\", 'to', 'ninety', 'percent', 'of', 'their', 'business', 'are', 'still', 'search', ',', \"'\", 'and', 'that', \"'AI\", 'doesn', '’', 't', 'really', 'move', 'the', 'needle', 'on', 'the', 'business', '.', \"'\", 'He', 'also', 'said', \"'the\", 'revenue', 'mix', '[', 'attributable', 'to', 'AI', ']', 'in', 'those', 'business', 'are', 'insignificant', '.', \"'\", 'And', 'while', 'he', '’', 's', 'not', 'bullish', 'on', 'FAANG', '’', 's', ',', 'he', 'doe', 'say', 'that', 'the', 'one', 'FAANG', 'to', 'still', 'watch', 'out', 'for', 'is', 'Amazon', ',', 'simply', 'because', 'ecommerce', 'still', 'represents', 'a', 'small', 'portion', 'of', 'the', 'global', 'retail', 'market', ',', 'giving', 'the', 'company', 'room', 'to', 'grow', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyRL_rz0bfy3",
        "colab_type": "text"
      },
      "source": [
        "### 4) 불용어(Stopwords) 처리 및 불필요한 품사 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld9xPo0Bbfy3",
        "colab_type": "code",
        "outputId": "77b54b6a-9cbe-4d6e-879d-46110e6b470d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "stopwords = [] #불용어 배열\n",
        "\n",
        "# 추출 키워드 대상이 되는 품사 지정\n",
        "wanted_POS = ['NN','NNS','NNP','NNPS']\n",
        "\n",
        "# 추출 키워드 대상 품사가 아닌 토큰은 불용어로 등록\n",
        "for word in POS_tag:\n",
        "    if word[1] not in wanted_POS:\n",
        "        stopwords.append(word[0])\n",
        "\n",
        "# punctuation 을 불용어로 추가\n",
        "punctuations = list(str(string.punctuation))\n",
        "stopwords = stopwords + punctuations\n",
        "\n",
        "\n",
        "# 사용자 정의 토큰을 불용어로 추가\n",
        "stopwords_plus = ['t', 'isn']\n",
        "stopwords = stopwords + stopwords_plus \n",
        "stopwords = set(stopwords)\n",
        "\n",
        "\n",
        "processed_text = []\n",
        "for word in lemmatized_text:\n",
        "    if word not in stopwords:\n",
        "        processed_text.append(word)\n",
        "print(processed_text)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['stock', 'growth', 'future', 'Bill', 'Studebaker', 'founder', 'Chief', 'Investment', 'Officer', 'Robo', 'Global', 'Studebaker', \"'reallocation\", 'tech', 'stock', 'stock', 'stock', 'week', 'March', 'midst', 'Amazon', 'Studebaker', 'stock', 'market', \"'reallocation\", 'stock', 'money', 'founder', 'Chief', 'Investment', 'Officer', 'Robo', 'Global', 'Bill', 'Studebaker', 'Business', 'Insider', 'stock', 'Facebook', 'Apple', 'Amazon', 'Netflix', 'Google', 'March', 'trend', 'news', 'Facebook', 'data', 'scandal', 'Nasdaq', 'frenzy', 'Investors', 'news', 'stock', 'optimist', 'Studebaker', 'trade', 'month', 'lot', 'performance', 'attribution', 'stock', 'stock', 'market', 'gain', 'month', 'tech', 'company', 'market', \"'reallocation\", 'technology', 'part', 'market', 'trend', 'future', 'reallocation', 'trade', 'de-risking', 'month', 'month', 'stock', 'investor', 'law', 'number', 'cap', 'mean', 'investor', 'value', 'stock', 'macro', 'backdrop', 'growth', 'demand', 'Studebaker', 'run', 'intelligence', 'robotics', 'fund', 'asset', 'management', 'AI', 'robotics', 'area', 'growth', 'ETF', 'year', 'stock', 'span', 'March', 'point', 'intelligence', 'area', 'boost', 'Google', 'Amazon', 'Studebaker', 'sign', 'growth', 'FAANGs', 'percent', 'business', \"'AI\", 'needle', 'business', 'revenue', 'mix', 'attributable', 'AI', 'business', 'doe', 'Amazon', 'ecommerce', 'portion', 'market', 'company', 'room']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRI5BKe94Ta-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a4fc620f-1ad9-43f0-9225-49a92ddcb7e3"
      },
      "source": [
        "string.punctuation"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVmNyjaKbfy5",
        "colab_type": "text"
      },
      "source": [
        "### 5) Unique한 토큰 목록 생성\n",
        "\n",
        "그래프 생성을 위해서 Unique한 토큰 목록 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erJIafXibfy6",
        "colab_type": "code",
        "outputId": "d28f9548-a57a-44e8-fc2f-ff4c5eb4666e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "vocabulary = list(set(processed_text))\n",
        "print(vocabulary)"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['lot', 'portion', 'Insider', 'Nasdaq', 'needle', 'midst', 'investor', 'company', 'Investors', 'macro', 'asset', 'Bill', 'span', 'gain', 'doe', 'fund', 'Robo', 'data', 'year', 'law', 'mean', 'performance', 'backdrop', 'money', 'optimist', 'technology', 'scandal', 'demand', 'run', 'area', 'revenue', 'trade', 'news', 'part', 'business', 'founder', 'Netflix', 'FAANGs', 'stock', 'Apple', 'Facebook', 'mix', 'de-risking', 'Amazon', 'future', 'ecommerce', 'Studebaker', 'room', 'March', 'month', 'number', 'attributable', 'market', 'reallocation', 'cap', 'growth', 'percent', 'management', 'sign', \"'AI\", 'Chief', 'trend', 'attribution', 'point', 'Investment', 'robotics', 'ETF', 'AI', 'boost', 'Global', 'Officer', 'tech', 'week', \"'reallocation\", 'Business', 'frenzy', 'intelligence', 'Google', 'value']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY8re-9gbfy9",
        "colab_type": "text"
      },
      "source": [
        "### 6) 그래프 생성 \n",
        "\n",
        "TextRank is a graph based model, and thus it requires us to build a graph. Each words in the vocabulary will serve as a vertex for graph. The words will be represented in the vertices by their index in vocabulary list.  \n",
        "\n",
        "The weighted_edge matrix contains the information of edge connections among all vertices.\n",
        "I am building wieghted undirected edges.\n",
        "\n",
        "weighted_edge[i][j] contains the weight of the connecting edge between the word vertex represented by vocabulary index i and the word vertex represented by vocabulary j.\n",
        "\n",
        "If weighted_edge[i][j] is zero, it means no edge connection is present between the words represented by index i and j.\n",
        "\n",
        "There is a connection between the words (and thus between i and j which represents them) if the words co-occur within a window of a specified 'window_size' in the processed_text.\n",
        "\n",
        "The value of the weighted_edge[i][j] is increased by (1/(distance between positions of words currently represented by i and j)) for every connection discovered between the same words in different locations of the text. \n",
        "\n",
        "The covered_coocurrences list (which is contain the list of pairs of absolute positions in processed_text of the words whose coocurrence at that location is already checked) is managed so that the same two words located in the same positions in processed_text are not repetitively counted while sliding the window one text unit at a time.\n",
        "\n",
        "The score of all vertices are intialized to one. \n",
        "\n",
        "Self-connections are not considered, so weighted_edge[i][i] will be zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phYnQIcBbfy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "vocab_len = len(vocabulary)\n",
        "\n",
        "weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
        "\n",
        "score = np.zeros((vocab_len),dtype=np.float32)\n",
        "window_size = 3\n",
        "covered_coocurrences = []\n",
        "\n",
        "for i in range(0,vocab_len):\n",
        "    score[i]=1\n",
        "    for j in range(0,vocab_len):\n",
        "        if j==i:\n",
        "            weighted_edge[i][j]=0\n",
        "        else:\n",
        "            for window_start in range(0,(len(processed_text)-window_size)):\n",
        "                \n",
        "                window_end = window_start+window_size\n",
        "                \n",
        "                window = processed_text[window_start:window_end]\n",
        "                \n",
        "                if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
        "                    \n",
        "                    index_of_i = window_start + window.index(vocabulary[i])\n",
        "                    index_of_j = window_start + window.index(vocabulary[j])\n",
        "                      \n",
        "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
        "                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
        "                        covered_coocurrences.append([index_of_i,index_of_j])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su4n0Q7VbfzA",
        "colab_type": "text"
      },
      "source": [
        "### Calculating weighted summation of connections of a vertex\n",
        "\n",
        "inout[i] will contain the sum of all the undirected connections\\edges associated withe the vertex represented by i."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVJt5k1mbfzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inout = np.zeros((vocab_len),dtype=np.float32)\n",
        "\n",
        "for i in range(0,vocab_len):\n",
        "    for j in range(0,vocab_len):\n",
        "        inout[i]+=weighted_edge[i][j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX6biajkbfzC",
        "colab_type": "text"
      },
      "source": [
        "### Scoring Vertices\n",
        "\n",
        "The formula used for scoring a vertex represented by i is:\n",
        "\n",
        "score[i] = (1-d) + d x [ Summation(j) ( (weighted_edge[i][j]/inout[j]) x score[j] ) ] where j belongs to the list of vertieces that has a connection with i. \n",
        "\n",
        "d is the damping factor.\n",
        "\n",
        "The score is iteratively updated until convergence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g-G1HwxbfzD",
        "colab_type": "code",
        "outputId": "6a273024-f7ef-491a-8a46-4053b6018e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "MAX_ITERATIONS = 50\n",
        "d=0.85\n",
        "threshold = 0.0001 #convergence threshold\n",
        "\n",
        "for iter in range(0,MAX_ITERATIONS):\n",
        "    prev_score = np.copy(score)\n",
        "    \n",
        "    for i in range(0,vocab_len):\n",
        "        \n",
        "        summation = 0\n",
        "        for j in range(0,vocab_len):\n",
        "            if weighted_edge[i][j] != 0:\n",
        "                summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
        "                \n",
        "        score[i] = (1-d) + d*(summation)\n",
        "    \n",
        "    if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n",
        "        print(\"Converging at iteration \"+str(iter)+\"....\")\n",
        "        break\n"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converging at iteration 31....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpMCtX24bfzG",
        "colab_type": "code",
        "outputId": "b85ebdb5-8226-4144-e147-db62326880d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1457
        }
      },
      "source": [
        "for i in range(0,vocab_len):\n",
        "    print(\"Score of \"+vocabulary[i]+\": \"+str(score[i]))"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score of lot: 0.6848181\n",
            "Score of portion: 0.6340738\n",
            "Score of Insider: 0.6298209\n",
            "Score of Nasdaq: 0.8359115\n",
            "Score of needle: 0.7524147\n",
            "Score of isn: 0.7430971\n",
            "Score of midst: 0.6222233\n",
            "Score of investor: 1.2873713\n",
            "Score of company: 0.8264576\n",
            "Score of Investors: 0.8021918\n",
            "Score of macro: 0.6596142\n",
            "Score of asset: 0.75754374\n",
            "Score of Bill: 1.0430568\n",
            "Score of span: 0.6415029\n",
            "Score of gain: 0.59739095\n",
            "Score of doe: 0.68117535\n",
            "Score of fund: 0.73801804\n",
            "Score of Robo: 1.0141851\n",
            "Score of data: 0.75828475\n",
            "Score of year: 0.65219885\n",
            "Score of law: 0.7377731\n",
            "Score of mean: 0.76092434\n",
            "Score of performance: 0.6963699\n",
            "Score of backdrop: 0.6586655\n",
            "Score of money: 0.59004873\n",
            "Score of optimist: 0.60736376\n",
            "Score of technology: 0.6115385\n",
            "Score of scandal: 0.8069137\n",
            "Score of demand: 0.6338073\n",
            "Score of run: 0.6435892\n",
            "Score of area: 1.2065072\n",
            "Score of revenue: 0.77174926\n",
            "Score of trade: 1.157185\n",
            "Score of news: 1.2283493\n",
            "Score of part: 0.61795616\n",
            "Score of business: 1.9873168\n",
            "Score of founder: 1.0434319\n",
            "Score of Netflix: 0.63783586\n",
            "Score of FAANGs: 0.69459456\n",
            "Score of stock: 5.1145606\n",
            "Score of Apple: 0.6438391\n",
            "Score of Facebook: 1.2298644\n",
            "Score of mix: 0.7741355\n",
            "Score of de-risking: 0.56017566\n",
            "Score of Amazon: 2.196201\n",
            "Score of future: 1.0961461\n",
            "Score of ecommerce: 0.65328807\n",
            "Score of Studebaker: 3.4840298\n",
            "Score of room: 0.15\n",
            "Score of March: 1.6795504\n",
            "Score of month: 1.7318987\n",
            "Score of number: 0.77964574\n",
            "Score of attributable: 0.76325786\n",
            "Score of market: 2.4417927\n",
            "Score of reallocation: 0.64434344\n",
            "Score of cap: 0.7822015\n",
            "Score of growth: 2.1417248\n",
            "Score of percent: 0.7194172\n",
            "Score of management: 0.7503899\n",
            "Score of sign: 0.6255047\n",
            "Score of 'AI: 0.7466382\n",
            "Score of Chief: 1.0334834\n",
            "Score of trend: 1.1278832\n",
            "Score of attribution: 0.5916894\n",
            "Score of point: 0.6572874\n",
            "Score of Investment: 1.0224285\n",
            "Score of robotics: 1.2770423\n",
            "Score of ETF: 0.65223634\n",
            "Score of AI: 1.3464209\n",
            "Score of boost: 0.6466147\n",
            "Score of Global: 1.0123613\n",
            "Score of Officer: 1.0168686\n",
            "Score of tech: 0.9784587\n",
            "Score of week: 0.61782557\n",
            "Score of 'reallocation: 1.5174197\n",
            "Score of Business: 0.61703706\n",
            "Score of frenzy: 0.8337142\n",
            "Score of intelligence: 1.2163965\n",
            "Score of Google: 1.1417611\n",
            "Score of value: 0.68098927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHWSpulWbfzI",
        "colab_type": "text"
      },
      "source": [
        "### Phrase Partiotioning\n",
        "\n",
        "Paritioning lemmatized_text into phrases using the stopwords in it as delimeters.\n",
        "The phrases are also candidates for keyphrases to be extracted. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGyVMVbubfzJ",
        "colab_type": "code",
        "outputId": "70178d3c-5620-4161-f2f1-6e94e85047de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "phrases = []\n",
        "\n",
        "phrase = \" \"\n",
        "for word in lemmatized_text:\n",
        "    \n",
        "    if word in stopwords:\n",
        "        if phrase!= \" \":\n",
        "            phrases.append(str(phrase).strip().split())\n",
        "        phrase = \" \"\n",
        "    elif word not in stopwords:\n",
        "        phrase+=str(word)\n",
        "        phrase+=\" \"\n",
        "\n",
        "print(phrases)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Partitioned Phrases (Candidate Keyphrases): \n",
            "\n",
            "[['stock'], ['growth'], ['future'], ['Bill', 'Studebaker'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global'], ['Studebaker'], [\"'reallocation\"], ['tech', 'stock'], ['stock'], ['stock'], ['week'], ['March'], ['midst'], ['Amazon'], ['Studebaker'], ['stock', 'market'], [\"'reallocation\"], ['stock'], ['money'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global', 'Bill', 'Studebaker'], ['Business', 'Insider'], ['stock'], ['Facebook'], ['Apple'], ['Amazon'], ['Netflix'], ['Google'], ['March'], ['trend'], ['news'], ['Facebook', 'data', 'scandal'], ['Nasdaq'], ['frenzy'], ['Investors'], ['isn'], ['news'], ['stock', 'optimist'], ['Studebaker'], ['trade'], ['month'], ['lot'], ['performance', 'attribution'], ['stock'], ['stock', 'market'], ['gain'], ['month'], ['tech', 'company'], ['market'], [\"'reallocation\"], ['technology'], ['part'], ['market'], ['trend'], ['future'], ['reallocation', 'trade'], ['de-risking'], ['month'], ['month'], ['stock'], ['investor'], ['law'], ['number'], ['cap'], ['mean'], ['investor'], ['value', 'stock'], ['macro', 'backdrop'], ['growth', 'demand'], ['Studebaker'], ['run'], ['intelligence'], ['robotics'], ['fund'], ['asset'], ['management'], ['AI'], ['robotics'], ['area'], ['growth'], ['ETF'], ['year'], ['stock'], ['span'], ['March'], ['point'], ['intelligence'], ['area'], ['boost'], ['Google'], ['Amazon'], ['Studebaker'], ['sign'], ['growth'], ['FAANGs'], ['percent'], ['business'], [\"'AI\"], ['needle'], ['business'], ['revenue', 'mix'], ['attributable'], ['AI'], ['business'], ['doe'], ['Amazon'], ['ecommerce'], ['portion'], ['market'], ['company', 'room']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw2kNJNxbfzM",
        "colab_type": "text"
      },
      "source": [
        "### Create a list of unique phrases.\n",
        "\n",
        "Repeating phrases\\keyphrase candidates has no purpose here, anymore. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RKObP9ebfzM",
        "colab_type": "code",
        "outputId": "a0b780d9-437d-40a6-e77a-8b0ab15d8731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "unique_phrases = []\n",
        "\n",
        "for phrase in phrases:\n",
        "    if phrase not in unique_phrases:\n",
        "        unique_phrases.append(phrase)\n",
        "\n",
        "print(unique_phrases)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique Phrases (Candidate Keyphrases): \n",
            "\n",
            "[['stock'], ['growth'], ['future'], ['Bill', 'Studebaker'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global'], ['Studebaker'], [\"'reallocation\"], ['tech', 'stock'], ['week'], ['March'], ['midst'], ['Amazon'], ['stock', 'market'], ['money'], ['Robo', 'Global', 'Bill', 'Studebaker'], ['Business', 'Insider'], ['Facebook'], ['Apple'], ['Netflix'], ['Google'], ['trend'], ['news'], ['Facebook', 'data', 'scandal'], ['Nasdaq'], ['frenzy'], ['Investors'], ['isn'], ['stock', 'optimist'], ['trade'], ['month'], ['lot'], ['performance', 'attribution'], ['gain'], ['tech', 'company'], ['market'], ['technology'], ['part'], ['reallocation', 'trade'], ['de-risking'], ['investor'], ['law'], ['number'], ['cap'], ['mean'], ['value', 'stock'], ['macro', 'backdrop'], ['growth', 'demand'], ['run'], ['intelligence'], ['robotics'], ['fund'], ['asset'], ['management'], ['AI'], ['area'], ['ETF'], ['year'], ['span'], ['point'], ['boost'], ['sign'], ['FAANGs'], ['percent'], ['business'], [\"'AI\"], ['needle'], ['revenue', 'mix'], ['attributable'], ['doe'], ['ecommerce'], ['portion'], ['company', 'room']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie8lFuS4bfzO",
        "colab_type": "text"
      },
      "source": [
        "### Thinning the list of candidate-keyphrases.\n",
        "\n",
        "Removing single word keyphrases-candidates that are present multi-word alternatives. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp467DtebfzP",
        "colab_type": "code",
        "outputId": "2369d0c8-0a0b-402a-d555-7a3f6c87060e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "for word in vocabulary:\n",
        "    #print word\n",
        "    for phrase in unique_phrases:\n",
        "        if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n",
        "            #if len(phrase)>1 then the current phrase is multi-worded.\n",
        "            #if the word in vocabulary is present in unique_phrases as a single-word-phrase\n",
        "            # and at the same time present as a word within a multi-worded phrase,\n",
        "            # then I will remove the single-word-phrase from the list.\n",
        "            unique_phrases.remove([word])\n",
        "            \n",
        "print(\"Thinned Unique Phrases (Candidate Keyphrases): \\n\")\n",
        "print(unique_phrases)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thinned Unique Phrases (Candidate Keyphrases): \n",
            "\n",
            "[['future'], ['Bill', 'Studebaker'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global'], [\"'reallocation\"], ['tech', 'stock'], ['week'], ['March'], ['midst'], ['Amazon'], ['stock', 'market'], ['money'], ['Robo', 'Global', 'Bill', 'Studebaker'], ['Business', 'Insider'], ['Apple'], ['Netflix'], ['Google'], ['trend'], ['news'], ['Facebook', 'data', 'scandal'], ['Nasdaq'], ['frenzy'], ['Investors'], ['isn'], ['stock', 'optimist'], ['month'], ['lot'], ['performance', 'attribution'], ['gain'], ['tech', 'company'], ['technology'], ['part'], ['reallocation', 'trade'], ['de-risking'], ['investor'], ['law'], ['number'], ['cap'], ['mean'], ['value', 'stock'], ['macro', 'backdrop'], ['growth', 'demand'], ['run'], ['intelligence'], ['robotics'], ['fund'], ['asset'], ['management'], ['AI'], ['area'], ['ETF'], ['year'], ['span'], ['point'], ['boost'], ['sign'], ['FAANGs'], ['percent'], ['business'], [\"'AI\"], ['needle'], ['revenue', 'mix'], ['attributable'], ['doe'], ['ecommerce'], ['portion'], ['company', 'room']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHbAUAfFbfzR",
        "colab_type": "text"
      },
      "source": [
        "### Scoring Keyphrases\n",
        "\n",
        "Scoring the phrases (candidate keyphrases) and building up a list of keyphrases\\keywords\n",
        "by listing untokenized versions of tokenized phrases\\candidate-keyphrases.\n",
        "Phrases are scored by adding the score of their members (words\\text-units that were ranked by the graph algorithm)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4DP76OsbfzR",
        "colab_type": "code",
        "outputId": "4e90479d-c29f-4445-dabe-65b5e7441b8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1241
        }
      },
      "source": [
        "phrase_scores = []\n",
        "keywords = []\n",
        "for phrase in unique_phrases:\n",
        "    phrase_score=0\n",
        "    keyword = ''\n",
        "    for word in phrase:\n",
        "        keyword += str(word)\n",
        "        keyword += \" \"\n",
        "        phrase_score+=score[vocabulary.index(word)]\n",
        "    phrase_scores.append(phrase_score)\n",
        "    keywords.append(keyword.strip())\n",
        "\n",
        "i=0\n",
        "for keyword in keywords:\n",
        "    print(\"Keyword: '\"+str(keyword)+\"', Score: \"+str(phrase_scores[i]))\n",
        "    i+=1"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keyword: 'future', Score: 1.0961461067199707\n",
            "Keyword: 'Bill Studebaker', Score: 4.527086615562439\n",
            "Keyword: 'founder', Score: 1.0434318780899048\n",
            "Keyword: 'Chief Investment Officer', Score: 3.07278048992157\n",
            "Keyword: 'Robo Global', Score: 2.026546359062195\n",
            "Keyword: ''reallocation', Score: 1.517419695854187\n",
            "Keyword: 'tech stock', Score: 6.0930193066596985\n",
            "Keyword: 'week', Score: 0.6178255677223206\n",
            "Keyword: 'March', Score: 1.6795504093170166\n",
            "Keyword: 'midst', Score: 0.6222233176231384\n",
            "Keyword: 'Amazon', Score: 2.1962010860443115\n",
            "Keyword: 'stock market', Score: 7.556353330612183\n",
            "Keyword: 'money', Score: 0.5900487303733826\n",
            "Keyword: 'Robo Global Bill Studebaker', Score: 6.553632974624634\n",
            "Keyword: 'Business Insider', Score: 1.2468579411506653\n",
            "Keyword: 'Apple', Score: 0.6438391208648682\n",
            "Keyword: 'Netflix', Score: 0.6378358602523804\n",
            "Keyword: 'Google', Score: 1.141761064529419\n",
            "Keyword: 'trend', Score: 1.1278831958770752\n",
            "Keyword: 'news', Score: 1.2283493280410767\n",
            "Keyword: 'Facebook data scandal', Score: 2.795062780380249\n",
            "Keyword: 'Nasdaq', Score: 0.8359115123748779\n",
            "Keyword: 'frenzy', Score: 0.8337141871452332\n",
            "Keyword: 'Investors', Score: 0.8021917939186096\n",
            "Keyword: 'isn', Score: 0.7430971264839172\n",
            "Keyword: 'stock optimist', Score: 5.721924364566803\n",
            "Keyword: 'month', Score: 1.7318986654281616\n",
            "Keyword: 'lot', Score: 0.6848180890083313\n",
            "Keyword: 'performance attribution', Score: 1.2880592942237854\n",
            "Keyword: 'gain', Score: 0.5973909497261047\n",
            "Keyword: 'tech company', Score: 1.8049163222312927\n",
            "Keyword: 'technology', Score: 0.6115385293960571\n",
            "Keyword: 'part', Score: 0.6179561614990234\n",
            "Keyword: 'reallocation trade', Score: 1.8015283942222595\n",
            "Keyword: 'de-risking', Score: 0.5601756572723389\n",
            "Keyword: 'investor', Score: 1.287371277809143\n",
            "Keyword: 'law', Score: 0.7377731204032898\n",
            "Keyword: 'number', Score: 0.7796457409858704\n",
            "Keyword: 'cap', Score: 0.7822015285491943\n",
            "Keyword: 'mean', Score: 0.7609243392944336\n",
            "Keyword: 'value stock', Score: 5.7955498695373535\n",
            "Keyword: 'macro backdrop', Score: 1.3182796835899353\n",
            "Keyword: 'growth demand', Score: 2.7755321264266968\n",
            "Keyword: 'run', Score: 0.643589198589325\n",
            "Keyword: 'intelligence', Score: 1.216396450996399\n",
            "Keyword: 'robotics', Score: 1.277042269706726\n",
            "Keyword: 'fund', Score: 0.7380180358886719\n",
            "Keyword: 'asset', Score: 0.7575437426567078\n",
            "Keyword: 'management', Score: 0.7503898739814758\n",
            "Keyword: 'AI', Score: 1.3464208841323853\n",
            "Keyword: 'area', Score: 1.2065072059631348\n",
            "Keyword: 'ETF', Score: 0.6522363424301147\n",
            "Keyword: 'year', Score: 0.652198851108551\n",
            "Keyword: 'span', Score: 0.6415029168128967\n",
            "Keyword: 'point', Score: 0.6572874188423157\n",
            "Keyword: 'boost', Score: 0.646614670753479\n",
            "Keyword: 'sign', Score: 0.6255046725273132\n",
            "Keyword: 'FAANGs', Score: 0.6945945620536804\n",
            "Keyword: 'percent', Score: 0.7194172143936157\n",
            "Keyword: 'business', Score: 1.9873168468475342\n",
            "Keyword: ''AI', Score: 0.7466381788253784\n",
            "Keyword: 'needle', Score: 0.7524147033691406\n",
            "Keyword: 'revenue mix', Score: 1.5458847284317017\n",
            "Keyword: 'attributable', Score: 0.7632578611373901\n",
            "Keyword: 'doe', Score: 0.6811753511428833\n",
            "Keyword: 'ecommerce', Score: 0.6532880663871765\n",
            "Keyword: 'portion', Score: 0.634073793888092\n",
            "Keyword: 'company room', Score: 0.9764576256275177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fw0Je8EbfzU",
        "colab_type": "text"
      },
      "source": [
        "### Ranking Keyphrases\n",
        "\n",
        "Ranking keyphrases based on their calculated scores. Displaying top keywords_num no. of keyphrases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpdzhcBBbfzV",
        "colab_type": "code",
        "outputId": "d705c959-25b8-4064-c28a-53a2cc0f731e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "sorted_index = np.flip(np.argsort(phrase_scores),0)\n",
        "\n",
        "keywords_num = 10\n",
        "\n",
        "print(\"Keywords:\\n\")\n",
        "\n",
        "for i in range(0,keywords_num):\n",
        "    print(str(keywords[sorted_index[i]])+\", \")"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keywords:\n",
            "\n",
            "stock market, \n",
            "Robo Global Bill Studebaker, \n",
            "tech stock, \n",
            "value stock, \n",
            "stock optimist, \n",
            "Bill Studebaker, \n",
            "Chief Investment Officer, \n",
            "Facebook data scandal, \n",
            "growth demand, \n",
            "Amazon, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpFwfuGlpFL9",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 gensim Textrank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW7gy7_7ooKb",
        "colab_type": "code",
        "outputId": "0ebd04e8-3efd-49b7-dc87-55d7604f6f8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "from gensim.summarization import keywords\n",
        "\n",
        "keywords(Text).split('\\n')"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['stocks',\n",
              " 'stock',\n",
              " 'studebaker',\n",
              " 'trade',\n",
              " 'trades',\n",
              " 'amazon',\n",
              " 'tech',\n",
              " 'attribution',\n",
              " 'attributable',\n",
              " 'cap',\n",
              " 'facebook',\n",
              " 'market',\n",
              " 'future',\n",
              " 'growth',\n",
              " 'thinks',\n",
              " 'think',\n",
              " 'frenzy',\n",
              " 'investment',\n",
              " 'big',\n",
              " 'global',\n",
              " 'favorable macro']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ar-QV1MoyGL",
        "colab_type": "code",
        "outputId": "8f67806d-715c-4af1-f49b-c2c01ab87504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "from gensim.summarization.summarizer import summarize\n",
        "\n",
        "print(summarize(Text))"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The FAANG stocks won’t see much more growth in the near future, according to Bill Studebaker, founder and Chief Investment Officer of Robo Global.\n",
            "Studebaker argues we are seeing a 'reallocation' that will continue from large-cap tech stocks into market-weight stocks.\n",
            "The stock market is seeing a 'reallocation' out of FAANG stocks, which are not where the smart money is, founder and Chief Investment Officer of Robo Global Bill Studebaker told Business Insider.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}